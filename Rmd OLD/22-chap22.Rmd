# Generalised Linear Mixed-Effects Models. {#chap22}

## Introduction

Recall the structure of the General Linear Model:

\[\mathbf{Y} = \mathbf{X} \beta + \epsilon, \epsilon \sim N(0, \sigma^2 \mathbf{I})\]

where __Y__ is a vector of responses, __X__ is a design matrix, $\beta$ is a vector of model parameters (intercepts, slopes, differences between treatments, etc.) and $\epsilon$ is a constant, $\ge 0$. __I__ is the identity matrix.

So far, we have extended this model in two different directions, in order to a) develop statistical methods for analysing data that come from different distributions (ie not Gaussian), and b) the incorporation of "random effects."

For a), we have the Generalised Linear Model (GLM):

\[g(\mu) = \mathbf{X} \beta, \mathbf{Y_i} \sim f(\mu, \theta)\]

where $g(.)$ is a link function and $f(\mu, \theta)$ is a probability distribution, with expectation $\mu$ and possibly some other parameters ($\theta$). Other terms as defined, above.

For b), we have the Linear Mixed-Effects Model (LME)"
\[\textbf{Y} = \mathbf{X\beta} + \mathbf{Zb} +  \mathbf{\epsilon}, \, \mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I}) ,\, \mathbf{b} \sim N(0, \delta^2\mathbf{I})\]

where __Z__ is a design matrix for the random effects, __b__ are the random effects, or best linear unbiased predictors (BLUPS). The BLUPs are assumed to be Normally distributed with some variance $\delta^2$. Note that more complex __Z__ matrices are possible and there might be different random effects at different (nested) scales of an experiment, or there might be crossed random effects, for example, or some more complicated sampling design.

## Generalised Linear Mixed-Effects Models

Combining the properties of the above two modelling approaches, we arrive at a more general approach that contains the above models as special cases: The Generalised Linear Mixed-Effects Model (GLMM). It has the form:

\[g(\mathbf{\mu}) = \mathbf{X\beta+Zb},\; \mathbf{Y_i} \sim f(\mathbf{\mu, \theta}), \; \mathbf{b} \sim N(0, \sigma^2\mathbf{I})\]

While this model is the most general that we have seen so far, it is also the most difficult to analyse. Importantly, to estimate the model parameters (fixed effects $\beta$ and also $\sigma$) we need to integrate over the random effects (__b__). In general, this integral is not available in closed form, so numerical approximations are necessary.

## Example: Logistic Regression on presence/absence data for birds
Consider a bird survey where 50 sites are visited and the presence (1) or absence (0) of a particular bird species is recorded. Each site is visited twice. Also, the distance to the nearest fresh water body is recorded (0: near, 1: far) The model is then:

\[\log \left( \frac{p_{ij}}{1-p_{ij}} \right) = \beta_0 + \beta_1 x_{ij} +  b_i, \; Y_{ij} \sim Bernoulli(p_{ij}), \; b_i \sim N(0, \sigma^2)\]

With $i$ the indicator for each site, and $j$ the indicator for the sampling times. $x$ is the binary variable indicating proximity to water. We are interested in estimating $\theta = (\beta_0, \beta_1, \sigma)$. The Bernoulli distribution is a special case of the binomial distribution, with $n = 1$ trial. It is commonly used to model presence/absence data.

The likelihood of the $\theta$ parameters is:
\[ L(\theta) = \int g(\mathbf{b};\theta) d \mathbf{b}\]

where, \[\displaystyle g(\mathbf{b}, \theta) = \prod_{i=1}^m \prod_{j=1}^{m_i} Pr(Y_{ij} = y_{ij} \mid b_i, \theta) \phi(b_i)\]
The double $\Pi$ sign is the product taken over all seasons and all sites. $\phi(b_i)$ is the probability density function (pdf) of the Normal distribution, as here we are assuming that the $b_i$'s are Normally distributed. We CANNOT evaluate this integral analytically!!! We need to find some method of approximation.

## GLMM in R

There are many packages that can do GLMMs in R. Three common ones are $\texttt{lme4}$ (using the $\texttt{glmer}$ function), $\texttt{glmmTMB}$, and $\texttt{GLMMadaptive}$. Each has their strengths and use different methods of approximating the integral, with different assumptions. Perhaps the most popular is the $\texttt{glmer}$ function in package $\texttt{lme4}$. We shall compare this to the $\texttt{GLMMadaptive}$ package and also to $\texttt{glmmTMB}$. First, get the data file [birds.csv](./birds.csv).

```{r, size='tiny', message=FALSE}
dat <- read.csv("birds.csv")
library(DHARMa)
library(lme4)
fit.glmer <- glmer(birdPresAbs ~ water + (1|site), data=dat, family=binomial)
summary(fit.glmer)
res <- simulateResiduals(fit.glmer, plot=TRUE)
```

Compare this to the summary output from $\texttt{GLMMadaptive}$:

```{r, message=FALSE}
library(GLMMadaptive)
fit.mm <- mixed_model(birdPresAbs ~ water, random=~1|site, data=dat, family=binomial)
summary(fit.mm)
res <- simulateResiduals(fit.mm, plot=TRUE)
```
And with glmmTMB:
```{r}
library(glmmTMB)
fit.tmb <- glmmTMB(birdPresAbs ~ water + (1|site), family=binomial, data=dat)
summary(fit.tmb)
res <- simulateResiduals(fit.tmb, plot=TRUE)
```
By default, $\texttt{glmer}$ and $\texttt{glmmTMB}$ use a Laplace approximation to the integral, whereas $\texttt{GLMMadaptive}$ uses a more involved technique, Adaptive Gaussian Quadrature. $\texttt{glmer}$ can use AGQ too. In fact, single-point Adaptive Gaussian Quadrature is equivalent to the Laplace approximation. (See this [paper by Pinheiro and Chao](./27594165.pdf)). (This paper is more than you will want to know about AGQ.) Note that the three packages give different answers, even when the same arguments are used for each method. Which is correct? Probably none of them!

## The Laplace approximation

![](./Laplace.jpg)

Pierre-Simon, marquis de Laplace (1749 - 1827) was known as "The French Newton." He worked in physics and mathematics and established the field of Bayesian statistics. (It should really be called "Laplacian statistics.") He predicted the existence of black
holes. When Napoleon asked him about God, he said "I have no need of that hypothesis."

The Laplace approximation is used by $\texttt{glmmTMB}$ and is the default method for the commonly-used $\texttt{glmer}$ function. It is easy to get an intuitive understanding of how it works. we will demonstrate it with a simple, flawed, example.

The aim of the method is to find the area under a curve. In our case, this curve would ordinarily be the likelihood function. However, likelihood functions are unwieldy and complicated. Instead, we will find the area under the curve of a cubic polynomial function. The outline of the algorithm is:

1. Find the maximum (mode) of the function
2. Taylor-expand the log-likelihood around this maximum to estimate the curvature (variance) of the function.
3. Approximate the function by a Gaussian (Normal) distribution with mean equal to the mode and variance equal to the Taylor series variance.
4. The area under the curve is the normalising constant of this Gaussian distribution.

## More Laplace details

* We wish to solve the following integral: $Z_P \equiv \int P^*(x) dx$.
* Find the maximum of the function $P^*(x)$ using any method that works. One method is the __Newton-Raphson__ algorithm. (see below)
* The Taylor series expansion of the log-likelihood around this maximum, $x_0$ is:
\[\log P^*(x) \simeq \log P^*(x_0) - \frac{c}{2}(x-x_0)^2 + \dots\]
, where
\[c= - \frac{\partial^2}{\partial x^2} \left. \log P^*(x)  \right|_{x=x_0}\]
* We approximate $P^*(x)$ by an unnormalized Gaussian,
      \[Q^*(x) \equiv P^*(x_0) \exp\left[- \frac{c}{2}(x-x_0)^2 \right]\]
(By unnormalized, we mean that the area under the Gaussian curve is $\ne 1$)
* Finally, we approximate the normalizing constant ($Z_P$) by the normalizing constant of this Gaussian,
      \[Z_Q = P^*(x_0) \sqrt{ \frac{2 \pi}{c}}\]
      
## The Newton-Raphson method

We need to find the maximum of the log-likelihood function ($x_0$), above. If we are lucky, we can do this analytically but more often, this is hard to do so we use numerical methods. The Newton-Raphson method is one simple method for finding the roots of a function.

*   Remember that the roots of the derivative of a function tell us the maxima and minima of the original function.
  1.    Compute the derivative of our function $g(x)$ with respect to $x$
  2.    Then use the Newton-Raphson algorithm to calculate the roots of this derivative:
  +   Make a guess at the solution
  +   Iterate $x_{j+1} = x_j - \frac{g(x_j)}{g'(x_j)}$ until convergence
  
## Newton-Raphson Example
  
* Say our function to integrate is $G(x) = x^3 − 5x^2 − 25x + 125$
* The derivative of this function is just $G'(x) = g(x) = 3x^2 − 10x − 25$ using high-school level calculus.
* The derivative of $g(x)$ is $g'(x) = 6x -10$
* Say our initial guess ($x_1$) is -20.
+ Our formula is: $x_{j+1} = x_j − \frac{g(x_j)}{g'(x_j)}$
Therefore,
+ x2 = −9.423077
+ x3 = −4.379169
+ x4 = −2.275157
+ x5 = −1.713632, x6 = −1.666993, x7 = −1.666667, x8 = −1.666667
* So the algorithm has converged to -1.666667 (-5/3).

Try writing an R function to implement the Newton-Raphson method for this cubic function.

## Getting the Answer Analytically

Because our cubic function $G(x)$ is so simple, we can easily find the maxima and minima using simple high-school maths. Setting $g(x) = 0$ and solving for $x$ we have:

\begin{align}
g(x) &= 3x^2 − 10x − 25 = 0 \\
0 &= 3x^2 − 15x + 5x − 25 \\
0 &= 3x(x − 5) + 5(x − 5) \\
0 &= (3x + 5)(x − 5), \therefore x = \frac{-5}{3}, x = 5 \\
\end{align}
There is a minimum at $x = 5$ and a maximum at $x=-5/3$. Using the Newton-Raphson method, you can get the $x = 5$ answer by using a large positive starting value (e.g. $x_1 = 20$).

Here is a graph of our cubic function showing the maximum and minimum:
```{r}
library(ggplot2)
Gfunc <- function (x) x^3 - 5 * x^2 - 25 * x + 125
ggplot(data.frame(x=c(-7, 7)), aes(x)) + stat_function(fun = Gfunc) +
geom_vline(xintercept = c(-5, -5/3, 5)) + geom_hline(yintercept = 0)
```
You can see also see the two roots (where $G(x) = 0$) at $\pm 5$. We wish to approximate the integral of $G(x)$ between the 2 roots. Using the N-R algorithm we have a maximum at
$x_0 = −5/3$. The Laplace approximation is $G(x_0) = \sqrt{\frac{2\pi}{c}}$ where:

\[c = - \frac{\partial^2}{\partial x^2} \log G(x) \rvert_{x=x_0}\]

We find that c = 0.135. (Try doing this differentiation on your own!) Substituting this into the formula for the normalising constant, we can plot both our analytical
curve and the Laplace approximation on the same graph:
```{r}
Gfunc <- function (x) x^3-5*x^2 - 25*x + 125
ggplot(data.frame(x = c(-6, 6)), aes(x)) + stat_function(fun = Gfunc) +
stat_function(fun = function(x) Gfunc(-5 / 3)*sqrt(2 * pi / 0.135) *
dnorm(x, mean= -5 / 3, sd = sqrt(1 / 0.135)), colour = "red") + 
geom_hline(yintercept = 0) +
geom_vline(xintercept=c(5, -5, -5/3), colour=c("black", "black", "orange"))
```
Finally, let’s compare the integrals directly over the range [-5, 5]:
$G(x) = x^3-5x^2 - 25x + 125$
\begin{align}
  \int^5_{-5}G(x)dx &= \left[ 1/4x^4-5/3x^3-25/2x^2+125x\right]^5_{-5} \\
      &= (1/4*25^4-5/3*5^3-25/2*5^2+125*5) - \\ &(1/4*-5^4-5/3*-5^3-25/2*-5^2+125*-5) \\
    &= 260.4167 - -572.9167 \\
    &= 833.3333
  \end{align}
```{r}  
Gfunc(-5 / 3) * sqrt(2 * pi / 0.135) * 
(pnorm(5, mean = -5/3, sd = sqrt(1/0.135)) 
-  pnorm(-5, mean = -5/3, sd=sqrt(1/0.135)))
```
We can see that the Laplace approximation severely overestimates the area under the curve between $\pm 5$. Inspecting the graph, the Laplace approximation (the red Normal curve) greatly overestimates the area on the left side of the curve and also to some extent on the right side of the curve (looking only between $x=\pm 5$). This is due to the assymetry (the "skewness") of the cubic equation between these points.

We can also use a simple form of quadrature to compare the results. (This is not the same as Adaptive Gaussian Quadrature that is used in the $\texttt{glmer}$ and $\texttt{GLMMadaptive}$ packages.)

```{r}
integrate(Gfunc, -5, 5)
```
We see that in this case, quadrature gives a very good result. 

## Summary

Fitting GLMMs requires approximations because we need to integrate over the random effects to get the fixed-effects model parameter estimates. These integrals are in general intractable.  One common approximation is Laplace's method. We can also use the Newton-Raphson method to determine the maxima of the likelihood function. The normalising constant of the approximating Gaussian is the value for the integral. The Laplace approximation is very fast to compute, and very flexible in that it can be used for complicated random effects structures in GLMMs. The Laplace approximation doesn't always work, especially if the likelihood is bounded or skewed. Transforming your data, different link functions or reparameterising your model may help. Other approaches include Adaptive Gaussian Quadrature, although this does not always work either. AGQ is a good approach but computationally difficult in all but the simplest cases, that is, with only one random effect (ie one variance parameter). Caveat Emptor!
