[["chap7.html", "7 LinearModels 7.1 Graphing the Iris data 7.2 Good Resources for Graphing 7.3 Analysing the iris data 7.4 Model Diagnostics 7.5 Analysis using t test 7.6 Analysis using ANOVA 7.7 Analysis using Regression 7.8 Points to Note 7.9 Summary 7.10 Linear Models 7.11 Matrix Notation for Linear Models 7.12 Matrix Representation 7.13 Distributional Assumptions 7.14 Estimation: Ordinary Least Squares", " 7 LinearModels Consider R. A. Fisher’s classic Iris data set, consisting of several morphological measurements on the flowers of three species of Iris. This data set is famous in statistics and is often used as test data for a wide variety of statistical methods. Fisher was probably the most important statistician and evolutionary biologist of the 20th century. He layed the foundations for our modern practice of statistics and he was a great evolutionary geneticist, partly responsible for inventing population genetics and establishing that natural selection could indeed be a powerful force in evolution. He was also a terrible grump and had lots of fights with other scientists. Although I like to find out about my idols, Fisher is someone that I am glad not to have met! Back to the data: knitr::include_graphics(&quot;sepalsPetals.jpg&quot;) Figure 7.1: An Iris knitr::include_graphics(&quot;fisher-smiling-50.jpg&quot;) Figure 7.2: R. A. Fisher data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Here we have loaded the data frame called iris It is built into R, which is convenient for us. The head() and tail() functions examine the first 6 and the last 6 lines of the data frame, allowing us to see the variable names and check that the data frame is usable. Although knitr::include_graphics(&quot;Kosaciec_szczecinkowaty_Iris_setosa.jpg&quot;) Figure 7.3: Iris setosa knitr::include_graphics(&quot;220px-Iris_virginica.jpg&quot;) Figure 7.4: Iris virginica knitr::include_graphics(&quot;220px-Iris_versicolor_3.jpg&quot;) Figure 7.5: Iris versicolor summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Here we have a six-figure summary of each of the variables: the mean, minimum and maximum values, plus the median, and first and 3rd quartiles. Together these numbers tell us a little about the distribution of the data (but not much). 7.1 Graphing the Iris data library(ggplot2) ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_point() + ylim(0, 8) Here we have a plot of the Petal Length for each species. The dots overlap a lot and it is hard to see what is going on, although you can see the maxima and minima quite clearly for each species. ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_jitter(width = 0.4, height = 0) + ylim(0, 8) Here are the same data but we have “jittered” the data along the x-axis, allowing us to see each data point separately from the others. This is a good way to display small amounts of data. Note that you should only jitter the data in the x direction in this case. Jittering the y data is wrong! (why?) ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_boxplot() + ylim(0, 8) If we have a larger amount of data, a box and whisker plot can be useful. It plots the median and interquartile range, and the outer whiskers extend to 1.5 x the size of the interquartile range. Extreme outliers are also indicated. 7.2 Good Resources for Graphing We won’t have much time to go into advanced graphing techniques in this course, although I will try to highlight graphing methods as we go along. Fortunately, there are some good books to consult. They are online through the library: R Graphics Cookbook by Chang ggplot2 Elegant Graphics for Data Analysis by Wickham 7.3 Analysing the iris data How would you analyse these data? Since we have 3 distinct groups (species) to compare and the data are continuous and (probably) normally-distributed, the Analysis of Variance (ANOVA) might be your first choice. Here is an analysis for petal length in R: fit &lt;- aov(Petal.Length ~ Species, data = iris) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that there is evidence for significant variation in petal length among species. We reject the null hypothesis that there are no differences in petal length among the three species. Note that this is a very vague conclusion. We don’t know which species differ in their petal length from the others, and we don’t know how big the differences really are. A thorough analysis would establish this information. 7.4 Model Diagnostics When building a statistical model, checking the model using diagnostic plots is a crucial step. All models have assumptions and if these assumptions are violated then the model will potentially give wrong results. For the ANOVA model, there are several important assumptions: The data are independent The residuals follow a Normal distribution The variance of the residuals is constant over the range of the data (homoscedastic errors). While all three assumptions are important, the first and third are crucial. It turns out that the Normality of residuals is not an overly strong assumption for the ANOVA model, but you should still check it, as large departures from Normality can cause problems. I usually rely on graphical approaches to testing assumptions: par(mfrow=c(1,2)) plot(fit, which = 1:2) The first diagnostic plot above, plots the residuals versus their fitted values. If the assumption of homoscedasticity holds (try saying that 5 times fast!), there will be no pattern to the residuals. If you see a curve, either upwards or downwards, that is suggestive of curvature in your data. If there is a “trumpet” like effect either increasing to the right or left, or you have a “bow tie” shape to the residual plot ie narrower in the middle, then the data are not homoscedastic. Subset and Recode the Data} iris2 &lt;- subset(iris, Species != &quot;setosa&quot;) Hypothesis: There exists a difference between the species’ means. How would you analyse these data? - t-test? - ANOVA? - Regression? - All three!!! 7.5 Analysis using t test fit.ttest &lt;- t.test(Petal.Length ~ Species, data = iris2, var.equal = TRUE) print(fit.ttest) ## ## Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0 ## 95 percent confidence interval: ## -1.495426 -1.088574 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 7.6 Analysis using ANOVA fit.anova &lt;- aov(Petal.Length ~ Species, data = iris2) summary(fit.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 1 41.73 41.73 158.9 &lt;2e-16 *** ## Residuals 98 25.74 0.26 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.7 Analysis using Regression fit.regression &lt;- lm(Petal.Length ~ Species, data=iris2) summary(fit.regression) ## ## Call: ## lm(formula = Petal.Length ~ Species, data = iris2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.260 -0.360 0.044 0.340 1.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.26000 0.07248 58.77 &lt;2e-16 *** ## Speciesvirginica 1.29200 0.10251 12.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5125 on 98 degrees of freedom ## Multiple R-squared: 0.6185, Adjusted R-squared: 0.6146 ## F-statistic: 158.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 7.8 Points to Note For an F test with one degree of freedom in the numerator, \\(F = t^2\\) with the denominator degrees of freedom for the F statistic equal to the degrees of freedom for the t statistic. We can get the regression output from the ANOVA fit using summary.lm summary.lm(fit.anova) ## ## Call: ## aov(formula = Petal.Length ~ Species, data = iris2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.260 -0.360 0.044 0.340 1.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.26000 0.07248 58.77 &lt;2e-16 *** ## Speciesvirginica 1.29200 0.10251 12.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5125 on 98 degrees of freedom ## Multiple R-squared: 0.6185, Adjusted R-squared: 0.6146 ## F-statistic: 158.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 Regression Plot iris2$coded &lt;- ifelse(iris2$Species == &quot;versicolor&quot;, 0, 1) options(warn = -1) mns &lt;- tapply(iris2$Petal.Length, iris2$coded, mean) df &lt;- data.frame(Petal.Length = mns, coded = 0:1, lower=c(NA, mns[1]), upper = c(NA, mns[2])) ggplot(data=iris2, aes(coded, Petal.Length)) + geom_point() + geom_smooth(method = lm , color = &quot;red&quot;, se = FALSE, formula=y~x) + geom_errorbar(data = df, mapping = aes(x = coded, ymin = lower, ymax = upper), width = 0.05, col = &quot;blue&quot;) + ylim(2.5, 7.5) + xlim(0, 1) + geom_text(label = paste(expression(beta[1])), x = 0.5, y = 4.5, parse = TRUE) + geom_text(label = paste(expression(beta[1])), x = 0.95, y = 5, parse = TRUE) + geom_text(label = paste(expression(beta[0])), x = 0.05, y = 4, parse = TRUE) + geom_text(label = paste(expression(height == beta[0] + beta[1] %*% Species)), x = 0.5, y = 7, parse = TRUE) 7.9 Summary If we code our categorical variables as 0, 1, then the slope \\(\\beta_1\\) is the same as the difference between the means for each category. The intercept \\(\\beta_0\\) is the value for the baseline category. With 3 or more categories, we construct the 0, 1 dummy variables: cat.var &lt;- LETTERS[1:3] model.matrix(~cat.var) ## (Intercept) cat.varB cat.varC ## 1 1 0 0 ## 2 1 1 0 ## 3 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cat.var ## [1] &quot;contr.treatment&quot; 7.10 Linear Models Comparing groups or doing regression are examples of the General Linear Model. Regression: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\epsilon \\sim NID(0, \\sigma^2)\\) ANOVA: \\(y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\epsilon \\sim NID(0, \\sigma^2)\\) Why ``Linear?’’ Definition: A linear transformation (equation) is defined by these 2 properties: – \\(f(x + y) = f(x) + f(y)\\): Additivity – \\(f(Ax) = Af(x)\\) : Homogeneity – Idea: The whole is equal to the sum of its parts. 7.11 Matrix Notation for Linear Models \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\epsilon_i \\sim NID(0, \\sigma^2)\\) \\[ Y_1 = \\beta_0 + \\beta_1 X_1 + \\epsilon_1 \\\\ Y_2 = \\beta_0 + \\beta_1 X_2 + \\epsilon_2 \\\\ Y_3 = \\beta_0 + \\beta_1 X_3 + \\epsilon_3 \\\\ \\vdots \\qquad \\vdots \\qquad \\vdots \\\\ Y_n = \\beta_0 + \\beta_1 X_n + \\epsilon_n \\] \\[ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} \\beta_0 + \\beta_1X_1 \\\\ \\beta_0 + \\beta_1X_2 \\\\ \\beta_0 + \\beta_1X_3 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1X_n \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] 7.12 Matrix Representation \\[ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_1 \\\\ 1 &amp; X_2 \\\\ 1 &amp; X_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_n \\end{bmatrix} \\times \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] \\[ \\mathbf{Y}= \\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon} \\]\\ \\[ \\mathbf{X}= \\begin{bmatrix} 1 &amp; X_1 \\\\ 1 &amp; X_2 \\\\ 1 &amp; X_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_n \\end{bmatrix} \\] Here we have \\(X\\) which is called the design matrix. \\[ \\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} \\] \\[ \\mathbf{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] \\[ \\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} \\] 7.13 Distributional Assumptions \\[ \\sigma^2_\\epsilon = \\sigma^2\\mathbf{I} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} \\] \\[ \\epsilon \\sim N(0, \\sigma^2\\mathbf{I}) \\] Full model: \\(\\textbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\epsilon}, \\quad \\mathbf{\\epsilon} \\sim N(0, \\sigma^2\\mathbf{I})\\) 7.14 Estimation: Ordinary Least Squares \\[ \\hat{\\beta} = (\\mathbf{X^TX})^{-1}\\mathbf{X^T} \\mathbf{Y} \\\\ \\hat{\\sigma^2} = \\frac{1}{n - p}(\\mathbf{Y} - \\mathbf{X\\hat{\\beta}})^T (\\mathbf{Y} - \\mathbf{X\\hat{\\beta}}) \\] where \\(\\hat{\\sigma}^2\\) is the unbiased estimator of \\(\\sigma^2\\), and \\(p=2\\) since we are estimating the slope and the intercept. \\(n\\) is the sample size. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
