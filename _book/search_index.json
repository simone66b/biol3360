[["index.html", "BIOL3360 Welcome to BIOL3360!", " BIOL3360 Simone Blomberg Welcome to BIOL3360! "],["chap1.html", "1 BIOL3360: Analysis and Communication of Biological Data 1.1 Course description 1.2 Course Delivery 1.3 Textbooks and Resources 1.4 Assessment 1.5 Goals and expectations 1.6 Course communication 1.7 And finally…", " 1 BIOL3360: Analysis and Communication of Biological Data 1.1 Course description As scientists, we need to be able to collect, analyse and interpret data in terms of statistical and mathematical models. We also need to be able to communicate our findings to other scientists, grant funding bodies, government departments and the general public. This course is designed to give you these skills. We intend that the skills you gain in this course will serve you well in your future careers and studies in biology, or indeed any other field of science. 1.2 Course Delivery The course is designed with three modules: Statistical Modelling, Dynamic Modelling and Communication. Statistical Modelling is taught by Simone Blomberg before the mid-semester break. The first three weeks are devoted to gaining R programming skills (no statistics taught in these weeks). Dynamic Modelling is taught by Jan Engelstaedter after the mid-semester break, and the Communication module is run for the entire semester by Louise Kuchel. The course is structured into two workshops, one two-hour and one three hour, for a total of 5 contact hours during the week. We expect you to spend another 5 hours on the content outside of contact hours. 1.3 Textbooks and Resources The textbook for R programming will be, The Art of R Programming by Matloff. This is a good introduction which emphasises good programming habits. We will work through several chapters. There is no set textbook for the Statistical Modelling module. However, there are many books which cover the material. My (Simone’s) favorites are: Mixed-Effects Modelling in S and S-PLUS Generalized Linear Models These books will cover the theory. We will discuss the theory and work through examples in R in the workshops. 1.4 Assessment There will be fortnightly quizzes throughout the course, and one practical exam for the communication module. There are no mid-semester or final exams. 1.5 Goals and expectations The course can be completed to a high level, if you work diligently. We expect you to come to the workshops (which will not necessarily be recorded). We expect you to prepare for each quiz. We expect you to use your initiative, intelligence, self-discipline, curiosity and vested interest to take responsibility for your own learning. If you start to have trouble, we expect you to ask for help from us, to get you back on track. Feel free to contact any of us by email, but do so sparingly, and be courteous and polite. Ask questions (publicly, privately, anonymously) if you don’t understand a concept. If we can improve something about our teaching or the course, let us know early because we will fix it if we can (SECaTs are good for us and for students next year, but they occur too late to benefit you). If you have a problem, let us know early and propose a solution if possible. 1.6 Course communication We will provide an Ed Discussion Board where you can ask questions of us, and your fellow students, anonymously if need be. Feel free to make an appointment to meet us personally outside of the workshops. We are happy to provide face-to-face advice. Our email addresses are: Simone s.blomberg1@uq.edu.au Jan j.engelstaedter@uq.edu.au Louise l.kuchel@uq.edu.au 1.7 And finally… Good luck with the course! We hope you enjoy it! Simone Blomberg. "],["chap2.html", "2 Week 1: R Programming I 2.1 Why Programming? Why Statistics? Why R? 2.2 Why Statistics? 2.3 Why R?", " 2 Week 1: R Programming I 2.1 Why Programming? Why Statistics? Why R? Computer programming has become an increasingly valued skill for biologists. Computer programming allows us to express complicated ideas in a formal way such that they can be analysed and evaluated using a computer. Thus, the computer keeps us honest with regard to our ideas. If we can’t explain our scientific problem to a computer, we haven’t thought it through well enough. Indeed, since Alan Turing showed that a computer can emulate any process that can be described by an algorithm, we might ask, “What are computers for?” Many of you will be used to thinking that computers are used for word processing, spreadsheets, email, Instagram etc. and that is true. But I think the answer to the question goes much deeper than this. The best answer that I can think of is, “Computers are machines for thinking with.” We use computers as an aid to thinking about the world. Computers can take the drudgery out of many scientific processes and leave us with room to think about the “big” questions. Computers are used in many situations in biology. In this course, we will be concerned with using computers to statistically and dynamically model aspects of data and biological processes. 2.2 Why Statistics? The biological world is messy. There is no way we can treat organisms the way atoms and molecules are treated in physics and chemistry. Even if we knew all the physical and chemical properties of all the molecules in an organism, we still could not predict accurately what that organism looks like or how it will behave so we need to use models that incorporate uncertainty and randomness in a principled way. This is why we do statistics. Statistical models have probability “built in.” Statistical models allow us to draw conclusions from data and form a way of thinking about organisms that takes randomness and uncertainty into account. Statistics is at the very heart of biology: the mechanism of natural selection requires that organisms differ, and differences among organisms are ultimately due to the random process of mutation. Mutation is necessary for selection to work. In eukaryotic cells, assortment of chromosomes occur at random, and there can be random “crossing over” events along a chromosome. Thus, statistical models help us to understand how and why organisms have evolved. We will be modelling the properties of data. Data are crucial to the scientific process and it is important to treat them fairly and gently. We will be using a variety of statistical methods to study data. From previous courses, you may be under the impression that statistics is about performing various tests on your data to draw conclusions. It is true that this is one aspect of statistics. But at a more fundamental level, we seek a good model for our data. After obtaining a good model, we find that the statistical tests are easy. They pretty much look after themselves. 2.3 Why R? R is a computer language, similar in many respects to other computer languages that you may have heard of, such as Python, C, and C++. There are many reasons why R is the language of choice for doing statistical modelling. It is free (both free as in “free beer” and free as in “free speech”). It was originally written by statisticians for statisticians so there are many aspects of the language that are ideally suited for doing statistical modelling. The language is relatively easy to read, write, and understand. There are thousands of add-on packages available for R that can be used to do many diverse forms of analysis. R has become the “lingua franca” of statistics. R is used by research statisticians to develop new methods, so chances are that you will have access to the latest methods for doing data analysis. Other packages can take many years to become up to date with well-known methods. Having a good knowledge of programming in R is a skill that employers will find valuable. Even if you don’t end up using R, and instead use some other software, you will find that learning a new system is made much easier because you already have experience with R. Like spoken languages, knowing one language can help you learn another. You can compare and contrast. Often you will find that other systems for statistical analysis are very inferior to R! "],["chap3.html", "3 Beginning R Programming 3.1 The Workspace Setup 3.2 First steps 3.3 The Art of R Programming", " 3 Beginning R Programming 3.1 The Workspace Setup Obtain R from the Comprehensive R Archive Network (CRAN) and install it in the usual way that you install other software. You can use R from the command line in a terminal by typing R (Mac and Linux) or by clicking on the R icon (Windows). However, using R in this way is usually quite tortuous. It is much better to use an Interactive Development Environment (IDE). We shall use RStudio as our IDE. Download and install RStudio from the Posit web site. Make sure you install the free version. After installation, start it up in the usual way for your system. An important point to note: RStudio is not R!. RStudio is merely an interface to R. You should normally not need to cite RStudio in reports etc. Just cite R. To cite R, type citation() at the R prompt. It will output a citation that you can paste into your reports. Open RStudio by clicking on the RStudio icon. A window should pop up with three panes. The pane on the left is the R console. This is where R output will appear. You can also type commands right in there. Minimise the R console and you should see another pane: This is the source pane. You can open new R documents there and edit your code. Make sure you save your code with the extension .R. That will allow RStudio to recognise your R file and give you text coloration and other goodies. The two panes on the right hand side of the window include facilities for examining your workspace environment, viewing plots and help files and a number of other things. Now we are ready to start programming! 3.2 First steps Go to the top-left pane and type. cat(&quot;Hello World!&quot;) Press the run button at the top right of the pane. You should see the output come out in the Console pane. Congratulations! This is your first R program. A “Hello World” program is traditionally the first program a new programmer writes on a new system that they are learning. You have joined the ranks of computer programmers! Some comments about your program: The program calls a function called cat which is a function that prints its arguments in the Console. “cat” stands for “concatenate.” You can use cat() to construct sentences from separate words, as the name suggests. In our case there was only one argument: “Hello World!” so that got printed. “Hello World!” is called a string (short for “character string”). So cat() concatenates strings together. Try: cat(&quot;I said,&quot;, &quot;Hello World!&quot;) You can see that the strings are concatenated and are printed out at the Console. I mentioned that cat() is a function. Functions are the building blocks of computer programs. You will be writing your own functions to do a variety of things. Inside the parentheses, you can put “arguments.” which is what the function works on. These are the inputs to the function. The function can have outputs, called “return values” and functions can also have side-effects. For cat(), the side-effect is to print out something at the Console prompt. The cat() function does not return a value. Functions have definitions. They are what you spend most of the time coding. A typical function definition is: mycat &lt;- function (str) { cat(&quot;This is a&quot;, str) } Notice that we are using the assignment character &lt;- to give a name to the function mycat(). Then follows the keyword function. The body of the function is enclosed within the curly braces. The function calls the cat() function and its argument is str. In this case, as cat() doesn’t return a value, neither will our function. Here is a test: result &lt;- mycat(&quot;dog&quot;) ## This is a dog result ## NULL Note that the function has a side-effect because cat() has a side-effect. (Printing the sentence). Note also that mycat() doesn’t return a value (actually the value NULL which is “nothing” in R.) 3.3 The Art of R Programming I have learned many languages such as: BASIC, Pascal, Lisp, R, MATLAB, and Julia. I also have some knowledge of FORTRAN and C. Mostly, I am self taught. I have found that the best way to learn a new computer language is to do two things. Get a good tutorial book. A good book is your friend. Working through the book will usually teach you everything you need to know, at least at a basic level. You can use the book for reference if you have forgotten how to do things. You can also look up new techniques as you become a better programmer. We use the Art of R Programming because it is a great technical introduction that will give you a good foundation to work from. Using a book, rather than relying on YouTube videos or Google means that you should get a well-rounded introduction to the language, examining all its important aspects evenly. However, should you find Google, YouTube, or even ChatGTP useful, then you should use them. Although the goal is to produce working code, in this course we will be encouraging you to test your understanding of code as well as your ability to write it. We will also be encouraging you to write well, so that you can produce code that is readable and easy to maintain. Have a programming project in mind. This can be as simple or as complex as you want. Although as a raw beginner, you should probably restrict yourself to very simple problems which demonstrate the various aspects and capabilities of the language. I encourage you to think up a project for yourself. You could work on interesting problems that have been raised in other previous courses, or something completely new and different. For example, a game or some piece of usable code that you will use in the future. It’s up to you! We will brainstorm ideas for projects in the workshops. Essentially, we will be having our own mini hackathon. Go to this padlet and post your ideas. Now to work! Go to “The Art of R Programming” and work through the text up to page 83. This will give you a good grounding in the basics of data types in R: Vectors, matrices, arrays, lists. Work through all the example code: Type it into the top left-hand pane and use the Run button to run your code. While you are reading, consider your own toy problem and how to apply R to it. Experiment along the way! Alternatively, you could work on this problem, which will give you some practice with R as you read through the chapters: A letter of the alphabet can be considered “odd” if its position in the alphabet is described by an odd number. If it is described by an even number, the letter is “even.” Write an R function that returns the number of odd letters and the number of even letters in a string of arbitrary length. Hint: There is a function called strsplit that you can use to split up a string into its constituent characters. See ?strsplit. "],["chap4.html", "4 More R Programming 4.1 Data Structures 4.2 Lists 4.3 Data Frames", " 4 More R Programming 4.1 Data Structures Data structures are fundamental to any computer language. You have already seen some: vectors, matrices, and arrays. In fact, matrices and arrays are really special types of vectors. Matrices and arrays have a special property: they possess dimensions. Try this: v &lt;- 1:9 v vmat &lt;- v; dim(vmat) &lt;- c(3, 3) vmat dim(vmat) &lt;- NULL vmat See that you can turn a vector into a matrix, and you can turn a matrix into a vector. The key is the dim() function which allows you to set and get the dimensions of a matrix or vector. The same applies to higher dimensional objects (arrays): avec &lt;- 1:16 arr &lt;- array(avec, c(4, 2, 2)) arr dim(arr) 4.2 Lists Vectors, matrices and arrays are of limited usefulness: they only allow the elements of these data structures to be of the same type. That is, vectors (and matrices and arrays) can only have one type of data in them. For example, you can have a vector of numbers: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 or a vector of characters: LETTERS[1:10] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; but you can’t have a vector that mixes characters and numbers: vec &lt;- c(1, &quot;A&quot;) vec ## [1] &quot;1&quot; &quot;A&quot; See that R turns the number (1) into a character (“1”). These are not the same thing! This behaviour may or may not be important to your program but it is crucial that you know about it. So how do you combine, say, characters and numbers? You use another data structure: the list. mylist &lt;- list(1, &quot;A&quot;) mylist ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;A&quot; Here we have a list with two elements. The first element is a numeric vector of length 1. It’s just the number 1. The second element is a character vector of length 1. It is just the character “A.” The way R prints out lists gives us a way to access elements, using the square bracket notation ([[, [, etc). So say we want the first element of mylist. We can do: mylist[[1]] ## [1] 1 Similarly for the second element. (Try it!) Another useful aspect of lists is that the elements can have names: names(mylist) &lt;- c(&quot;Number&quot;, &quot;Letter&quot;) mylist ## $Number ## [1] 1 ## ## $Letter ## [1] &quot;A&quot; mylist$Letter ## [1] &quot;A&quot; See that if the list has named elements, we can use the $ notation. This notation is less general: lists will always have an ordered set of elements but lists don’t always have names attached to the elements. The $ notation is more readable, however. Note also that unlike vectors and matrices, you can have “nested” lists: lists within lists. This is a very useful property in many circumstances: mylist &lt;- list(list(&quot;A&quot;, 1), list(&quot;B&quot;, 2)) mylist ## [[1]] ## [[1]][[1]] ## [1] &quot;A&quot; ## ## [[1]][[2]] ## [1] 1 ## ## ## [[2]] ## [[2]][[1]] ## [1] &quot;B&quot; ## ## [[2]][[2]] ## [1] 2 names(mylist) &lt;- c(&quot;Element1&quot;, &quot;Element2&quot;) mylist ## $Element1 ## $Element1[[1]] ## [1] &quot;A&quot; ## ## $Element1[[2]] ## [1] 1 ## ## ## $Element2 ## $Element2[[1]] ## [1] &quot;B&quot; ## ## $Element2[[2]] ## [1] 2 mylist[[1]][[2]] ## accesses the second element of Element1. ## [1] 1 mylist$Element1[[1]] ## mix and match! ## [1] &quot;A&quot; 4.3 Data Frames A data frame is a special type of list. Its main difference is that a data frame is a list with all its elements having the same length. This is the way that statistical data are usually represented: The columns of the data frame represent variables. The rows of the data frame represent observations. Thus, a data frame is “rectangular.” Here’s a simple example: library(ade4) data(lizards) dat &lt;- lizards$traits head(dat) ## mean.L matur.L max.L hatch.L hatch.m clutch.S age.mat clutch.F ## Sa 69.2 58 82 27.8 0.572 6.0 13 1.5 ## Sh 48.4 42 56 22.9 0.310 3.2 5 2.0 ## Tl 168.4 132 190 42.8 2.235 16.9 19 1.0 ## Mc 66.1 56 72 25.0 0.441 7.2 11 1.5 ## My 70.1 60 81 26.6 0.550 5.4 10 1.0 ## Pb 55.2 44 64 24.0 0.304 4.2 8 2.0 Here we have used the lizards data set from the ade4 package. This data set is actually a list: It has elements traits and two phylogenies: hprA and hprB I extracted the data frame from this list and looked at the first 6 lines. It is a data.frame with 8 variables (columns) and 18 observations, one for each species of lizard in the data set. Since dat is also a list, we can refer to the columns using the square-bracket notation or the $ notation. Also, data frames have another property: they have dimensions: dim(dat) ## [1] 18 8 And you can refer to the elements of the data frame as if they were a matrix: dat[3, 4] ## [1] 42.8 This gives the value at the 3rd row and 4th column. Although each column (variable) has to be of the same length, if you have missing data, you can just put NA which is the missing data character in R: dat[3, 4] &lt;- NA dat[1:4, 1:5] ## mean.L matur.L max.L hatch.L hatch.m ## Sa 69.2 58 82 27.8 0.572 ## Sh 48.4 42 56 22.9 0.310 ## Tl 168.4 132 190 NA 2.235 ## Mc 66.1 56 72 25.0 0.441 Usually you will import data from a file into a data.frame. You can have your NA values in the data file and R will understand them and import them with the rest of the data. "],["chap5.html", "5 Flow of Control 5.1 Loops 5.2 For loops 5.3 while and repeat 5.4 Branching code: The if-then-else construct. 5.5 Conclusion", " 5 Flow of Control Some programs move in a “linear” fashion, from the first statement, to the second, then third etc. until the end of the program is reached. However, most programs (ie functions) that are longer than 3 or 4 lines tend to allow R to jump around the program from one statement to another, but not in a linear order. For example, we may want to repeat a block of statements a large number of times. Or we may want the program to do different things, depending on the state of some variable in the program. 5.1 Loops The most common way of iterating over a block of statements is to use a loop. In R, there are three commands that provide loops, each with its different use. 5.2 For loops The most commonly used loop is the for loop. It is used when you want to iterate several statements over the elements of a vector. A simple example: x &lt;- 1:10 for (i in x) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 You can see that the function just prints out the square of the numbers from one to ten, one per line. Although it is most common to iterate over integers (x &lt;- 1:10), we can iterate over the contents of any vector: for (bed in c(&quot;mat&quot;, &quot;sofa&quot;, &quot;table&quot;)) { cat(&quot;The cat sat on the&quot;, bed, &quot;\\n&quot;) } ## The cat sat on the mat ## The cat sat on the sofa ## The cat sat on the table 5.3 while and repeat The next most commonly used loop is a while loop. You go around the loop until some condition is not met. Then you exit the loop and go onto the next statement. Here’s an example: foo &lt;- function () { ## No arguments! cat(&quot;Values from a Normal distribution\\n&quot;) x &lt;- rnorm(1) ## generate a value while (abs(x) &lt; 1.96) { cat(x, &quot;\\n&quot;) ## print x as a side-effect x &lt;- rnorm(1) ## generate a new value } ## go around the loop from here. cat(&quot;Outlier detected:&quot;, x, &quot;\\n&quot;) ## another side-effect } foo() ## call the function. ## Values from a Normal distribution ## Outlier detected: 2.457809 Note that the loop occurs while the test condition is TRUE. The flow of control breaks out of the while loop as soon as the test condition is FALSE. You can also break out of loops using the break command. You can also skip an iteration of the loop (usually a for loop) using the next command. The repeat loop is not often used. It simply iterates over a block of code, without the possibility of the loop ending. You can escape from a repeat loop using the break or next commands. 5.4 Branching code: The if-then-else construct. Frequently we want R to make decisions in our code that depend on some criterion. We want our program to change its behaviour depending on this. This is where the if-then-else construct comes in. Here is a simple example: testcat &lt;- function (animal) { if (animal == &quot;cat&quot;) { cat(&quot;Good kitty.\\n&quot;) } else { cat(&quot;What sort of animal are you?\\n&quot;) } } testcat(&quot;cat&quot;) ## Good kitty. testcat(&quot;dog&quot;) ## What sort of animal are you? Here we can see the if-then-else at work. The test criterion is if (animal == \"cat\") then do cat(\"Good kitty.\\n\") else do cat(\"What sort of animal are you?\\n\"). You can see that you don’t actually need to write the then. The if-then-else construct allows us to make programs that branch at one or several places in the code, and execute different blocks of code depending on the test criterion. You can also “chain” if-then-else statements together to test for multiple conditions in the code. 5.5 Conclusion Now we have the building blocks of an R function: We can assign the function a name, and we tell R it is a function using the function key word, followed by the arguments to the function. The main body of the function determines how the arguments are to be treated, which may involve loops or if-else constructs. Finally a value is returned by the function. Along the way, the function might have side-effects such as printing to the screen. "],["chap6.html", "6 Experimental Design 6.1 Some History 6.2 Experiments 6.3 More terminology 6.4 Randomness 6.5 Positive and Negative Controls 6.6 Blinding of experiments 6.7 Covariates 6.8 “Natural” Experiments or Natural “Experiments” 6.9 Random effects and Blocking 6.10 Power 6.11 Factors affecting the Power of a Test 6.12 Built-in R Functions for Power Analysis 6.13 Power of the t-test 6.14 Exercises 6.15 Package pwr 6.16 Exercises 6.17 Effect Sizes 6.18 Exercises 6.19 Power Curves 6.20 Power Analysis by Simulation 6.21 Exercise 6.22 References", " 6 Experimental Design 6.1 Some History Sir Ronald Fisher developed the foundations for the rigourous experimental design when he was a researcher at Rothamsted Experimental Station 1919 -1933. Rothamsted was an agricultural field station and there were a lot of studies conducted there on the effectiveness of different fertiliser treatments, for example. On arriving at Rothamsted, Fisher was struck by how badly the analyses of these field trials were conducted. He set about developing the theory of the Design of Experiments and he also developed statistical methods to correctly analyse the results of experiments. We use the principles laid down by Fisher to this day. Although Fisher had some wrong ideas about eugenics, the genetics of intelligence and racial differences, he was remarkable in that he almost single-handedly invented what we now know as classical statistics. and also developed the theory of population genetics (along with J. B. S. Haldane and Sewell Wright). He was a giant of 20th century science and an evolutionary biologist at heart. Unfortunately he was rather grumpy and prone to having disagreements and fights with most of his contemporaries. They say you should never meet your heroes and I am glad I never met Fisher. He died in 1962 in Adelaide. His ashes are interred in St. Peters Anglican cathedral in Adelaide. If you go to Adelaide you can visit Fisher’s ashes. Put your hand on the plaque and absorb some of Fisher’s force! 6.2 Experiments We first consider the simplest experiment: the comparison of two experimental treatments. What is a treatment? A treatment is some sort of manipulation carried out by a researcher on a group of experimental units. An example might be if we are trying to find out the effect of a drug, we may give the drug to some subjects. To contrast this treatment, we will give a placebo to another group of people. The delivery of the drug or placebo are both treatments. The experimental units are the subjects in the experiment. This simple study has all the properties of an experiment: There is greater than one treatment. (We have 2 treatments, drug and placebo). We are interested in whether the drug works in that the drug treatment is more effective than the placebo, so we can contrast the treatments. We have performed the experiment a number of times, giving either the drug or the placebo to several subjects. Finally, we do this at random, that is, each subject has a probability of 0.5 of receiving either the drug or the placebo. More on the subject of randomness below. You may have noticed that what we have described is a randomised, controlled trial This is the common term for experiments in the medical field and in the media. Calling a study an RCT is much easier to sell to the general public than an experiment. Subjects generally don’t want to be experimented on or treated as guinea pigs so the RCT terminology is used. Statisticians know that we are all talking about ``experiments.’’ 6.3 More terminology In our example, we can consider the two treatments (Drug and Placebo) to be properties of a unified object, the factor. We thus have a factor Drug with two levels: drug and placebo. In more complicated experiments, there may be more than one factor in an experiment, and the treatments are combinations of the levels of the different factors. Aside: When Fisher defined a factor, he was thinking about genetics, where different alleles were considered to be different genetic factors. We now use the term factor in a more general sense, beyond just allelic differences in phenotypes. The data we take from our subjects during the experiment are called the response or outcome variables of the experiment. For our drug example, if we were interested in the effect of the drug on blood pressure, then we would measure the blood pressure of each experimental unit (subject) after applying the drug or placebo. The blood pressure would be the response variable. The factor Drug (with levels drug and placebo) is called the explanatory variable and is defined by our experimental design. The difference between the effect of the drug treatment and the placebo is called the effect size. The placebo treatment is called a control treatment. Controls represent a treatment that contains everything about the drug treatment, except the drug itself. A placebo pill contains no active ingredients but application of a placebo can still affect the results (compared to not taking a pill at all) due to the placebo effect. The effect of taking the drug is thus isolated from all the other factors that might influence the results that occur both in the drug treatment and the placebo control. Any effect, the difference between the two treatments, must be due to the application of the drug alone. 6.4 Randomness The role of randomness crops up in two places in experiments. The first is that the experimental units differ in small ways from each other. On one hand we should try to use experimental units (subjects, in our case), that are as uniform as possible to reduce the effects of this randomness. This is the reason for using in-bred lines of rats, mice or rabbits in experiments as they are effectively clones with no genetic differences. They are more uniform, at least in their genetics. On the other hand, if our experiment is expected to be more natural we may be interested in running it on a more representative group of experimental units. For example, we may recruit subjects with different backgrounds, body size, gender, age, etc. So that any effects of the drug will be more like the representative effect seen in a clinical setting where lots of different people seek to take the same drug, but respond differently. Such considerations should lead you to think carefully about the hypothesis you are trying to test (more on hypothesis testing below). The other source of randomness is that deliberately induced by the experimenter: The treatments are assigned to the subjects at random, using a random number generator. The reasoning here is that such random allocation to treatments serves to break up any systematic differences that could possibly occur among subjects. In contrast, any other mechanism for allocating treatments to experimental units has the potential for systematic bias, which will render the experiment useless. An example might be if we ask subjects to arrive at the clinic at various times during the day, and we gave the drug to all the subjects that arrived early in the morning, and the placebo to the subjects who arrived at the clinic late in the day. If, for example, the effectiveness of the drug depended on the time it was taken (e.g. due to circadian rhythm effects), then our experiment would be destroyed because a systematic bias was introduced during treatment allocation. (We say that the drug effect is confounded with the time the drug was taken. Confounding is a big problem and you should really look out for it in the design of any experiment that you do.) Only random application of treatments to experimental units can avoid these systematic biases. 6.5 Positive and Negative Controls Most control treatments in experiments are usually negative in the sense that they include all the effects on a treatment, except the one of interest to the researcher. However, there are many situations where we need a positive control: A treatment where an effect is guaranteed to be detected. A common example is when biologists are seeking to sequence an unknown gene. In order to make sure the sequencing experiment is working, they include a gene of known sequence. If the experiment can’t get the correct result for the known sequence, then it is clear that there is something wrong with the experimental technique. 6.6 Blinding of experiments Blinding in experiments is a very useful technique for avoiding all sorts of systematic biases. The method is to apply the experimental treatment without the subject knowing what treatment they are being given. So in our example, the researcher gives a pill to the subjects without telling them whether the treatment is the drug or the placebo. Of course, the researcher knows this information so it can be used later in the statistical analysis. An even more sneaky method is the so-called double blind trial. Here, both the researcher and the subject do not know which treatment is being applied. The information is kept by a third party and only revealed to the experimenter at the time of the analysis when collecting of data is over. Doubly-blind trials are a powerful technique for controlling many sorts of systematic bias that may be inadvertently introduced by the experimenter and the subject. Sometimes it is not possible to perform such trials, however. 6.7 Covariates Another useful part of an experiment is to collect other data along with the response variable of interest. For example, in our drug experiment, we could collect information on the sex and body size of the subjects, their level of obesity, family history of heart disease etc. These are all of interest and may be important in contributing to the response of the subjects to the drug treatment. Yet, they are not being experimentally manipulated by the researcher. They are simply extra information that can be used in the interpretation of the experiment. They are called covariates. It is important to remember that perhaps the defining feature of an experiment is to manipulate the experimental units in some way. In this way we can distinguish cause and effect. However, it is possible to collect data on a lot of covariates and relate them to some other variable (the presumed outcome or response) without manipulating the study system and then derive conclusions about cause and effect. Such observational studies are fraught because they are heavily dependent on the underlying causal model being correct. However, there could easily be other unmeasured variables that are contributing to the outcome, or the measured covariates could be related to each other in complicated ways. Without experimentation, we cannot usually establish cause and effect. (There are some exceptions to this rule but they rely on extra assumptions and are beyond the scope of this course. If you want information about this, ask me in class.) 6.8 “Natural” Experiments or Natural “Experiments” Some people talk about “natural” experiments, such as those provided by natural disasters or weather events, when it is assumed that treatments have been applied to the experimental units at random by the mechanism of the extreme conditions. However, the quotes should be around “experiment” not natural, as nobody doubts the naturalness of the event but it is somewhat dubious to consider it as an experiment akin to researcher manipulated studies where treatments have been allocated to experimental units at random. Some systematic bias may have been introduced, of which we are not aware. 6.9 Random effects and Blocking One particularly useful technique in the design of experiments is the use of random effects, also known as blocking. The definition of random effects is slightly complicated so we will leave it to a later chapter. Here we will consider two examples, so you get the idea. Returning to our drug experiment, say we wanted the experiment to be quite large (lots of replicate subjects). So we decide to run the experiment at different clinic locations or hospitals. Now, it is possible that different clinics might produce slightly different results (drug effects). This might be due to the fact that subjects that attend one clinic might receive better treatment, more accurate doses of the drug, may live closer to one clinic rather than another. etc. The upshot is that some clinics will have, on average, higher or lower effect sizes than others. This variability due to the hospital effect leads to the idea of blocking, where we assign patients to blocks (clinics) at random to each of the blocks, The treatments (drug and control) are given to subjects who are a member of a particular clinic, and we use the information of which clinic was attended in the analysis. We then consider the variability in the response to among versus within block effects. So ultimately we are able to model the variation due to blocks and account for it as a component of the variation in the experiment. More on this in the chapter on mixed-effects models. Aside: The term “block” comes from agricultural trials where fields were divided up into blocks, based on the topography of the field. The second case where random effects and blocking is common is when we take multiple measurements from our subjects. This is called a repeated measures design. In our drug example, we may, in a sense, use each individual as their own control, by first measuring their blood pressure, applying the treatment (drug versus control) and then re-measuring their blood pressure. Here, the “block” is the individual subject. Naturally, some subjects will respond well to the drug, and some not so well. By treating subjects as blocks, we can remove this subject to subject variation and account for it in our analysis. In our simple drug example, we would probably use a paired t-test to test for differences between treatments, within subjects, which is a simple case of a repeated measures analysis. 6.10 Power I have said previously that we should choose to do our experiment on many experimental units. This is called replication. Then each experimental unit is also known as a replicate. But how many replicates do we need in each treatment? How big should our sample size be? In a series of papers in the early 20th century, Jerzey Neyman and Egon S. Pearson developed a decision-theoretic approach to hypothesis-testing. The theory was later extended and generalised by Abraham Wald. Neyman, Pearson and Wald. Wald was one of the first to notice the phenomenon of survival bias during World War II. A centrepiece of the Neyman-Pearson approach is the idea of there being two possible types of error when considering rejecting a hypothesis based on data: \\(H_0\\) False \\(H_0\\) True Reject \\(H_0\\) Correct Type I error Accept \\(H_0\\) Type II error Correct Thus, a rational approach to hypothesis testing will seek to reject a hypothesis if it is false and accept a hypothesis when it is true. The two types of error are rejecting the null hypothesis when it is true (Type I) and accepting the null hypothesis when it is false (Type II). In the Neyman-Pearson theory, it is usual to fix the Type I error probability (\\(\\alpha\\)) at some constant (often at 0.05, but not necessarily), and then choose a test which minimises the Type II error probability (\\(\\beta\\)), conditional on \\(\\alpha\\). The (null) hypothesis is then either rejected when the associated p-value for the test is less than \\(\\alpha\\), or otherwise accepted. The Neyman-Pearson theory has come under a lot of criticism since its formulation. Although much maligned, it is still used to justify and compare statistical testing procedures, whether or not scientists accept the paradigm for their everyday data analyses. For our purposes, the theory introduces the concept of “power” of a test, where power is defined as P(Reject \\(H_0 | H_0\\) is false). That is, 1 minus the Type II error probability, or \\(1 − \\beta\\). (The first person to promote power as a concept was “Student” a.k.a W. S. Gosset, head brewer for the Guinness brewery. (Interestingly, R. A. Fisher (the other main architect of classical statistics) was against the whole idea of power.) 6.11 Factors affecting the Power of a Test There are several factors affecting the power of a test: There is a trade-off between α and β such that greater power can be gained by accepting a less stringent condition for the Type I error. In other words, power is increased when α is increased. The shortcoming of setting a higher α is that Type I errors will be more likely. This may not be desirable. Power can be increased by increasing the effect size. The reasoning is that any test will have trouble rejecting the null hypothesis if the null hypothesis is only slightly wrong. If the effect size of an experiment (such as the difference between a treatment and a control) is large, then it is easier to detect, and the null hypothesis will be soundly rejected. The use of covariates or blocking variables can also increase the power of a test, as controlling for other, nuisance, variables can make the test more precise. Increasing the sample size in an experiment or observational study also increases the power to detect a response. It is important to carefully consider sample sizes before doing any experimental work. This is because data are expensive to collect, and may involve an appreciable amount of time and money expended by the researcher. Also, especially in medical research, there are ethical issues to consider. Experiments should be designed so that the minimum number of subjects (e.g. rats, dogs, or monkeys) are used in order to arrive at valid scientific conclusions. For these reasons, it is important to take a quantitative approach to maximising power in an experiment and to design experimental protocols that are efficient in their use of resources (subjects). 6.12 Built-in R Functions for Power Analysis R comes with a wide variety of built-in functions. For power analysis, we need to look to the stats package that comes with R. Try searching for functions for power analysis in the stats package. Your output should look approximately like this: help.search(&quot;power&quot;, package=&quot;stats&quot;) stats::power Create a Power Link Object Aliases: power stats::power.anova.test Power Calculations for Balanced One-Way Analysis of Variance Tests Aliases: power.anova.test stats::power.prop.test Power Calculations for Two-Sample Test for Proportions Aliases: power.prop.test stats::power.t.test Power calculations for one and two sample t tests Aliases: power.t.test stats::print.htest Print Methods for Hypothesis Tests and Power Calculation Objects Aliases: print.power.htest We can see that there are 5 functions with power in their name. However, it is clear that power.anova.test, power.prop.test, and power.t.test are the most appropriate for our study of power analysis. We first examine power.t.test. 6.13 Power of the t-test Recall that one use of the t-test is used to test for differences between two sample means, drawn from two Normal populations with unknown variance. The null hypothesis is that the samples were drawn from a single population. i.e. H0 : the sample means are not different. Now, examine the help page for power.t.test, part of which is presented below: ?power.t.test power.t.test package:stats R Documentation Power calculations for one and two sample t tests Description: Compute power of test, or determine parameters to obtain target power. Usage: power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05, power = NULL, type = c(&quot;two.sample&quot;, &quot;one.sample&quot;, &quot;paired&quot;), alternative = c(&quot;two.sided&quot;, &quot;one.sided&quot;), strict = FALSE) Notice that power.t.test accepts 8 arguments. Arguments type, alternative and strict describe the behaviour of the t-test. That is, whether the test is a two-sample, one-sample, or paired t-test, and whether it is a two-sided or one-sided test. Further, we are given the option of including both tails in the power calculation for the two-sided test, by setting strict=TRUE. Note that the default is strict=FALSE. The first 5 arguments determine the type of analysis. To use the function, you specify 4 of the first 5 arguments, and the unspecified argument is the one that is output from the computation. Here’s an example: power.t.test(n=6, .2, sd=.1, power=NULL) ## ## Two-sample t test power calculation ## ## n = 6 ## delta = 0.2 ## sd = 0.1 ## sig.level = 0.05 ## power = 0.8764176 ## alternative = two.sided ## ## NOTE: n is number in *each* group Note that we have left the sig.level argument at its default (0.05). We have specified the sample size, and the two arguments that contribute to the effect size (delta, the difference between the means and sd, the common standard deviation. More on effect sizes below). power was set to NULL, as this is the value we are trying to compute. The output is printed, including the estimated power (0.88 in this case). 6.14 Exercises Calculate the power of a two-sample, two-sided t-test when n=12, delta=1.5, sd=2.3. Use a significance level of 0.05. Using the the same t-test, calculate the sample size needed to attain a power of 0.85, with delta=6, sd=4.5. Use a significance level of 0.05. power.anova.test: Calculate the sample size needed for a one-way ANOVA with between-group variance = 3, within-group variance=5, and 4 groups. Use a significance level of 0.05. Calculate the power of a one-way ANOVA with 4 groups, within-group variance = 12, between-group variance=4, 20 subjects in each group. Use a significance level of 0.05. power.prop.test Calculate the power of the test comparing the proportions in two groups (0.5, 0.4), with 20 in each group. Use a significance level of 0.05. Calculate the sample size necessary to detect a difference in proportions where group 1 has a proportion of .6 and group 2 has a proportion of 0.8. Use power=0.85, and a significance level of 0.01. 6.15 Package pwr Load the pwr package into R using the following command: library(pwr) and then look at the diverse array of functions provided in the package: ES.h Effect size calculation for proportions ES.w1 Effect size calculation in the chi-squared test for goodness of fit ES.w2 Effect size calculation in the chi-squared test for association cohen.ES Conventional effects size plot.power.htest Plot diagram of sample size vs. test power pwr-package Basic Functions for Power Analysis pwr pwr.2p.test Power calculation for two proportions (same sample sizes) pwr.2p2n.test Power calculation for two proportions (different sample sizes) pwr.anova.test Power calculations for balanced one-way analysis of variance tests pwr.chisq.test power calculations for chi-squared tests pwr.f2.test Power calculations for the general linear model pwr.norm.test Power calculations for the mean of a normal distribution (known variance) pwr.p.test Power calculations for proportion tests (one sample) pwr.r.test Power calculations for correlation test pwr.t.test Power calculations for t-tests of means (one sample, two samples and paired samples) pwr.t2n.test Power calculations for two samples (different sizes) t-tests of means Notice that there are many more functions for power analysis in pwr than in built-in package stats. There are additional functions for \\(\\chi^2\\) tests (pwr.chisq.test), Pearson’s product-moment correlation (pwr.r.test), and unbalanced two-sample t-tests (pwr.t2n.test), among others. However, they work in the same way as the previous examples from stats. You specify the values for all the arguments except one (which is left as NULL), and that unspecified variable is computed by the function. 6.16 Exercises Calculate the power of the Pearson’s product moment correlation test, where r = 0.8, n = 20, and significance level is 0.05. Calculate the sample size necessary to detect a correlation of r = 0.6, with a power of 0.85 and significance level = 0.05. 6.17 Effect Sizes It was mentioned previously that increasing the effect size (the standardised “difference” between treatment groups) results in an increased power. However, calculation of effect sizes varies from test to test, depending on the underlying distribution of the test statistic. Frequently, we do not know the likely effect size that may occur in an experiment. The best approach is then to do a pilot experiment on a small scale to estimate the likely effect size. In the absence of pilot data, Cohen (1988) provides standard measures of effect size, classified as “small”, “medium”, and “large” for a variety of tests. These effect sizes are built into the pwr package, using the function cohen.ES. Although these “standard” effect sizes are somewhat arbitrary, they can provide a first guide for sample size estimation. Note, however, that a pilot experiment is the recommended way to estimate effect sizes for an experimental study. 6.18 Exercises Use cohen.ES to extract “small”, “medium”, and “large” effect sizes for \\(\\chi^2\\), Pearson’s r and proportion tests. Use the above effect sizes to calculate sample sizes with power = 0.8, and sig.level = 0.05, using the following functions from the pwr package: pwr.chisq.test, pwr.r.test, pwr.p.test. Calculate the power of the above tests with sample sizes 10, 50, 100. Calculate the detectable effect size for the above tests when power = 0.8 and n = 10, 50, 100. 6.19 Power Curves Calculating specific values of power, sample size or effect size can be illuminating with regard to the statistical restrictions on experimental design and analysis. But frequently a graph tells the story more completely and succinctly. Here we show how to draw power, sample size, and effect size curves using the above functions in R: nvals &lt;- seq(2, 100, length.out=200) powvals &lt;- sapply(nvals, function (x) power.t.test(n=x, delta=1)$power) plot(nvals, powvals, xlab=&quot;n&quot;, ylab=&quot;power&quot;, main=&quot;Power curve for\\n t-test with delta = 1&quot;, lwd=2, col=&quot;red&quot;, type=&quot;l&quot;) If we are unsure of our effect size, we can also alter delta to see the effect of both effect size and sample size on power: deltas &lt;- c(0.2, 0.4, 0.8) plot(nvals, seq(0,1, length.out=length(nvals)), xlab=&quot;n&quot;, ylab=&quot;power&quot;, main=&quot;Power Curve for\\nt-test with varying delta&quot;, type=&quot;n&quot;) for (i in 1:3) { powvals &lt;- sapply(nvals, function (x) power.t.test(n=x, delta=deltas[i])$power) lines(nvals, powvals, lwd=2, col=i) } legend(&quot;topleft&quot;, lwd=2, col=1:3, legend=c(&quot;0.2&quot;, &quot;0.4&quot;, &quot;0.8&quot;)) ## Exercises 13. Make a graph of the relationship between effect size and power, for a sample size of 5, 10, and 20, using power.anova.test. Use 4 groups, with the within-group variance equal to 1, and the between-group variance varying between 0.1 and 1.2. 6.20 Power Analysis by Simulation Frequently, the complexity of our experimental designs means that we must go far beyond what can be accomplished with standard software, such as the built-in power functions and the pwr package. Fortunately, R can be easily programmed to produce power analyses for any experimental design. The general approach is: Simulate data under the null hypothesis (for checking Type I error prob- abilities) and also for different effect sizes, to estimate power. Fit the model to the simulated data. Record whether the analysis of the simulated data set was significant, using the usual tests. Store the significance level in a vector. Repeat from step 1. a large number of times. Tabulate how many simulations produced a significant result, and hence calculate power. Here is an example: Suppose we wish to conduct a study with two fixed factors, leading us to a 2-way analysis of variance (ANOVA), with two levels for each factor. We could simulate data under the null hypothesis (no difference between means) using the following code: ## First set up the design reps &lt;- 1000 design &lt;- expand.grid(A=c(&quot;a&quot;, &quot;b&quot;), B=c(&quot;c&quot;,&quot;d&quot;), reps=1:10) pvals &lt;- vector(&quot;numeric&quot;, length=reps) ## simulate data under the null hypothesis. for (i in 1:reps) { design$response &lt;- rnorm(40) # random data with zero mean ## fit the model fit &lt;- aov(response~A*B, data=design) ## record the p-value for the interaction term. ## You could record other values. ## Save the p value pvals[i] &lt;- summary(fit)[[1]][[5]][3] } Type1 &lt;- length(which(pvals &lt; 0.05))/reps print(Type1) ## [1] 0.057 It appears that the Type I error rate is acceptable for the 2-factor ANOVA interaction term. Now to calculate some power statistics. We will calculate power for a difference between means of 2 units, for sample sizes 5, 10, 20. ssize &lt;- c(5, 10, 20) pvals &lt;- matrix(NA, nrow=reps, ncol=3) ## First set up the design for (j in 1:3) { reps &lt;- 1000 design &lt;- expand.grid(reps=1:ssize[j], A=c(&quot;a&quot;, &quot;b&quot;), B=c(&quot;c&quot;,&quot;d&quot;)) ## simulate data under the null hypothesis. for (i in 1:reps) { design$response &lt;- c(rnorm(3*ssize[j]), rnorm(ssize[j], mean=2)) ## fit the model fit &lt;- aov(response~A*B, data=design) ## record the p-value for the interaction term. ## You could record other values. ## Save the p value pvals[i,j] &lt;- summary(fit)[[1]][[5]][3] } } Power &lt;- apply(pvals, 2, function (x) length(which(x &lt; 0.05))/reps) names(Power) &lt;- as.character(ssize) print(Power) ## 5 10 20 ## 0.572 0.874 0.992 We see that the power is too low for a sample size of 5, but it increases to an acceptable level for 10 replicates per treatment. 6.21 Exercise Construct a graph of the relationship between power and sample size for a multiple regression model with 3 predictor variables, over a range of 1 to 10 for each predictor. For the effect size, let the residual error have \\(\\sigma\\) = 5, and \\(\\beta_1 = 0.1\\), \\(\\beta_2 = 1\\) and \\(\\beta_3 = 5\\). Try varying the effect size to examine its effects on power in this case. 6.22 References Cohen (1988) is the classic text for non-statisticians. Cohen J., 1988. Statistical Power Analysis for the Behavioural Sciences. Routledge, 2nd ed. Here in the library "],["chap7.html", "7 LinearModels 7.1 Graphing the Iris data 7.2 Good Resources for Graphing 7.3 Analysing the iris data 7.4 Model Diagnostics 7.5 Analysis using t test 7.6 Analysis using ANOVA 7.7 Analysis using Regression 7.8 Points to Note 7.9 Summary 7.10 Linear Models 7.11 Matrix Notation for Linear Models 7.12 Matrix Representation 7.13 Distributional Assumptions 7.14 Estimation: Ordinary Least Squares", " 7 LinearModels Consider R. A. Fisher’s classic Iris data set, consisting of several morphological measurements on the flowers of three species of Iris. This data set is famous in statistics and is often used as test data for a wide variety of statistical methods. Fisher was probably the most important statistician and evolutionary biologist of the 20th century. He layed the foundations for our modern practice of statistics and he was a great evolutionary geneticist, partly responsible for inventing population genetics and establishing that natural selection could indeed be a powerful force in evolution. He was also a terrible grump and had lots of fights with other scientists. Although I like to find out about my idols, Fisher is someone that I am glad not to have met! Back to the data: knitr::include_graphics(&quot;sepalsPetals.jpg&quot;) Figure 7.1: An Iris knitr::include_graphics(&quot;fisher-smiling-50.jpg&quot;) Figure 7.2: R. A. Fisher data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Here we have loaded the data frame called iris It is built into R, which is convenient for us. The head() and tail() functions examine the first 6 and the last 6 lines of the data frame, allowing us to see the variable names and check that the data frame is usable. Although knitr::include_graphics(&quot;Kosaciec_szczecinkowaty_Iris_setosa.jpg&quot;) Figure 7.3: Iris setosa knitr::include_graphics(&quot;220px-Iris_virginica.jpg&quot;) Figure 7.4: Iris virginica knitr::include_graphics(&quot;220px-Iris_versicolor_3.jpg&quot;) Figure 7.5: Iris versicolor summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Here we have a six-figure summary of each of the variables: the mean, minimum and maximum values, plus the median, and first and 3rd quartiles. Together these numbers tell us a little about the distribution of the data (but not much). 7.1 Graphing the Iris data library(ggplot2) ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_point() + ylim(0, 8) Here we have a plot of the Petal Length for each species. The dots overlap a lot and it is hard to see what is going on, although you can see the maxima and minima quite clearly for each species. ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_jitter(width = 0.4, height = 0) + ylim(0, 8) Here are the same data but we have “jittered” the data along the x-axis, allowing us to see each data point separately from the others. This is a good way to display small amounts of data. Note that you should only jitter the data in the x direction in this case. Jittering the y data is wrong! (why?) ggplot(aes(x = Species, y = Petal.Length), data = iris) + geom_boxplot() + ylim(0, 8) If we have a larger amount of data, a box and whisker plot can be useful. It plots the median and interquartile range, and the outer whiskers extend to 1.5 x the size of the interquartile range. Extreme outliers are also indicated. 7.2 Good Resources for Graphing We won’t have much time to go into advanced graphing techniques in this course, although I will try to highlight graphing methods as we go along. Fortunately, there are some good books to consult. They are online through the library: R Graphics Cookbook by Chang ggplot2 Elegant Graphics for Data Analysis by Wickham 7.3 Analysing the iris data How would you analyse these data? Since we have 3 distinct groups (species) to compare and the data are continuous and (probably) normally-distributed, the Analysis of Variance (ANOVA) might be your first choice. Here is an analysis for petal length in R: fit &lt;- aov(Petal.Length ~ Species, data = iris) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that there is evidence for significant variation in petal length among species. We reject the null hypothesis that there are no differences in petal length among the three species. Note that this is a very vague conclusion. We don’t know which species differ in their petal length from the others, and we don’t know how big the differences really are. A thorough analysis would establish this information. 7.4 Model Diagnostics When building a statistical model, checking the model using diagnostic plots is a crucial step. All models have assumptions and if these assumptions are violated then the model will potentially give wrong results. For the ANOVA model, there are several important assumptions: The data are independent The residuals follow a Normal distribution The variance of the residuals is constant over the range of the data (homoscedastic errors). While all three assumptions are important, the first and third are crucial. It turns out that the Normality of residuals is not an overly strong assumption for the ANOVA model, but you should still check it, as large departures from Normality can cause problems. I usually rely on graphical approaches to testing assumptions: par(mfrow=c(1,2)) plot(fit, which = 1:2) The first diagnostic plot above, plots the residuals versus their fitted values. If the assumption of homoscedasticity holds (try saying that 5 times fast!), there will be no pattern to the residuals. If you see a curve, either upwards or downwards, that is suggestive of curvature in your data. If there is a “trumpet” like effect either increasing to the right or left, or you have a “bow tie” shape to the residual plot ie narrower in the middle, then the data are not homoscedastic. Subset and Recode the Data} iris2 &lt;- subset(iris, Species != &quot;setosa&quot;) Hypothesis: There exists a difference between the species’ means. How would you analyse these data? - t-test? - ANOVA? - Regression? - All three!!! 7.5 Analysis using t test fit.ttest &lt;- t.test(Petal.Length ~ Species, data = iris2, var.equal = TRUE) print(fit.ttest) ## ## Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0 ## 95 percent confidence interval: ## -1.495426 -1.088574 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 7.6 Analysis using ANOVA fit.anova &lt;- aov(Petal.Length ~ Species, data = iris2) summary(fit.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 1 41.73 41.73 158.9 &lt;2e-16 *** ## Residuals 98 25.74 0.26 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.7 Analysis using Regression fit.regression &lt;- lm(Petal.Length ~ Species, data=iris2) summary(fit.regression) ## ## Call: ## lm(formula = Petal.Length ~ Species, data = iris2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.260 -0.360 0.044 0.340 1.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.26000 0.07248 58.77 &lt;2e-16 *** ## Speciesvirginica 1.29200 0.10251 12.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5125 on 98 degrees of freedom ## Multiple R-squared: 0.6185, Adjusted R-squared: 0.6146 ## F-statistic: 158.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 7.8 Points to Note For an F test with one degree of freedom in the numerator, \\(F = t^2\\) with the denominator degrees of freedom for the F statistic equal to the degrees of freedom for the t statistic. We can get the regression output from the ANOVA fit using summary.lm summary.lm(fit.anova) ## ## Call: ## aov(formula = Petal.Length ~ Species, data = iris2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.260 -0.360 0.044 0.340 1.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.26000 0.07248 58.77 &lt;2e-16 *** ## Speciesvirginica 1.29200 0.10251 12.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5125 on 98 degrees of freedom ## Multiple R-squared: 0.6185, Adjusted R-squared: 0.6146 ## F-statistic: 158.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 Regression Plot iris2$coded &lt;- ifelse(iris2$Species == &quot;versicolor&quot;, 0, 1) options(warn = -1) mns &lt;- tapply(iris2$Petal.Length, iris2$coded, mean) df &lt;- data.frame(Petal.Length = mns, coded = 0:1, lower=c(NA, mns[1]), upper = c(NA, mns[2])) ggplot(data=iris2, aes(coded, Petal.Length)) + geom_point() + geom_smooth(method = lm , color = &quot;red&quot;, se = FALSE, formula=y~x) + geom_errorbar(data = df, mapping = aes(x = coded, ymin = lower, ymax = upper), width = 0.05, col = &quot;blue&quot;) + ylim(2.5, 7.5) + xlim(0, 1) + geom_text(label = paste(expression(beta[1])), x = 0.5, y = 4.5, parse = TRUE) + geom_text(label = paste(expression(beta[1])), x = 0.95, y = 5, parse = TRUE) + geom_text(label = paste(expression(beta[0])), x = 0.05, y = 4, parse = TRUE) + geom_text(label = paste(expression(height == beta[0] + beta[1] %*% Species)), x = 0.5, y = 7, parse = TRUE) 7.9 Summary If we code our categorical variables as 0, 1, then the slope \\(\\beta_1\\) is the same as the difference between the means for each category. The intercept \\(\\beta_0\\) is the value for the baseline category. With 3 or more categories, we construct the 0, 1 dummy variables: cat.var &lt;- LETTERS[1:3] model.matrix(~cat.var) ## (Intercept) cat.varB cat.varC ## 1 1 0 0 ## 2 1 1 0 ## 3 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cat.var ## [1] &quot;contr.treatment&quot; 7.10 Linear Models Comparing groups or doing regression are examples of the General Linear Model. Regression: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\epsilon \\sim NID(0, \\sigma^2)\\) ANOVA: \\(y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\epsilon \\sim NID(0, \\sigma^2)\\) Why ``Linear?’’ Definition: A linear transformation (equation) is defined by these 2 properties: – \\(f(x + y) = f(x) + f(y)\\): Additivity – \\(f(Ax) = Af(x)\\) : Homogeneity – Idea: The whole is equal to the sum of its parts Linear models are the workhorse of all statistical modelling. They crop up everywhere. Especially, the computational ease with which we can analyse linear models governs the procedures for the design of experiments. The reason for the use of the guidelines for the design of experiments in Chapter 6 is precisely so that we can apply a linear modelling framework to the experimental system. This is why we need to spend so much time on them. An appreciation of the mathematics is very useful for understanding their uses, similarities and differences from each other. 7.11 Matrix Notation for Linear Models The mathematical formulation of a linear regression model (perhaps the simplest linear model) is below: \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\epsilon_i \\sim NID(0, \\sigma^2)\\) The \\(Y_i\\)’s are the values of the response variable. The \\(\\beta\\)’s are the intercept and slope parameters. It is these that we would like to estimate.. \\(X_i\\) are the data for the explanatory variable. \\(\\sigma^2\\) is the error variance: the variance of the residuals around the line. The residual is simply the distance between the data point and the fitted line, on the y axis. We can write down an equation for each data point: \\[ Y_1 = \\beta_0 + \\beta_1 X_1 + \\epsilon_1 \\\\ Y_2 = \\beta_0 + \\beta_1 X_2 + \\epsilon_2 \\\\ Y_3 = \\beta_0 + \\beta_1 X_3 + \\epsilon_3 \\\\ \\vdots \\qquad \\vdots \\qquad \\vdots \\\\ Y_n = \\beta_0 + \\beta_1 X_n + \\epsilon_n \\] We can form column vectors for the elements of each equation: \\[ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} \\beta_0 + \\beta_1X_1 \\\\ \\beta_0 + \\beta_1X_2 \\\\ \\beta_0 + \\beta_1X_3 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1X_n \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] 7.12 Matrix Representation We can then factor out the \\(\\beta\\) terms from the middle vector. \\[ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_1 \\\\ 1 &amp; X_2 \\\\ 1 &amp; X_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_n \\end{bmatrix} \\times \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] We can then write down the matrix equation for the linear model: \\[ \\mathbf{Y}= \\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon} \\] \\[ \\mathbf{X}= \\begin{bmatrix} 1 &amp; X_1 \\\\ 1 &amp; X_2 \\\\ 1 &amp; X_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_n \\end{bmatrix} \\mathbf{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} \\mathbf{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} \\] Here we have \\(\\mathbf{X}\\) which is called the design matrix. \\(\\beta\\) is the vector of parameters (to be estimated). \\(\\mathbf{\\epsilon}\\) is the vector of errors. \\(\\mathbf{Y}\\) is the vector of responses. In this, the General Linear Model, \\(\\mathbf{X}\\) can have more than 2 columns. The first column is usually a vector of 1’s (corresponding to the intercept). The other columns contain the explanatory variables, which may be continuous (leading to multiple regression) or discrete (usually 0, 1), leading to the Analysis of Variance (ANOVA). Mixtures of continuous and discrete explanatory variables lead to the Analysis of Covariance (ANCOVA) and other more complicated models. 7.13 Distributional Assumptions Finally, we have to specify the assumptions regarding the statistical distribution of the residuals. This will usually be one number, specifying that the variance of residuals, given the symbol \\(\\sigma^2\\). If we want to specify the variance for each data value, we can write: \\[ \\sigma^2_\\epsilon = \\sigma^2\\mathbf{I} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} \\] where \\(\\mathbf{I}\\) is the identity matrix. Thus, each data point has a variance of \\(\\sigma^2\\) and the off-diagonals are all zero, signifying that there is no correlation between the data points. We can write this as: \\[ \\epsilon \\sim N(0, \\sigma^2\\mathbf{I}) \\] We say that the epsilons (\\(\\mathbf{\\epsilon}\\)) are distributed as following a Normal distribution, with zero mean and variance \\(\\sigma^2\\mathbf{I}\\). N is the usual symbol for the Normal distribution and the “~” means “is distributed as.” This leads to the Full model: \\(\\Huge\\textbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\epsilon}, \\quad \\mathbf{\\epsilon} \\sim N(0, \\sigma^2\\mathbf{I})\\) This equation is worth remembering. 7.14 Estimation: Ordinary Least Squares We wish to estimate \\(\\beta\\) and \\(\\sigma^2\\). This is usually done by the method of Ordinary Least Squares (OLS). The derivation of these estimators is beyond the scope of the course. You may have seen the formula for \\(\\hat{\\beta}\\) before: it was the last exercise in the matrix problem sheet. \\[ \\hat{\\beta} = (\\mathbf{X^TX})^{-1}\\mathbf{X^T} \\mathbf{Y} \\\\ \\hat{\\sigma^2} = \\frac{1}{n - p}(\\mathbf{Y} - \\mathbf{X\\hat{\\beta}})^T (\\mathbf{Y} - \\mathbf{X\\hat{\\beta}}) \\] where \\(\\hat{\\sigma}^2\\) is the unbiased estimator of \\(\\sigma^2\\), and \\(p=2\\) in the situation where we are estimating the slope and the intercept. \\(n\\) is the sample size. "],["chap8.html", "8 Generalised Least Squares 8.1 Estimation: GLS 8.2 GLS example: Phylogenetic autocorrelation 8.3 GLS example: Phylogenetic autocorrelation 8.4 GLS example: Time Series 8.5 Lynx Data 8.6 ACF and PACF 8.7 Autoregression of Lynx Data 8.8 GLS Autoregression model of Lynx data", " 8 Generalised Least Squares In OLS, the assumption about the errors is that they are independent and identically distributed. What if they are not independent? One common example is that the errors are correlated with each other. Then we have: \\[ \\epsilon \\sim N(0, \\sigma^2 \\mathbf{\\Sigma}) \\] where \\(\\Sigma\\) is a variance-covariance matrix for the residuals. ie \\[ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma^2_1 &amp; Cov(\\epsilon_1, \\epsilon_2) &amp; \\cdots &amp; Cov(\\epsilon_1, \\epsilon_n) \\\\ Cov(\\epsilon_2, \\epsilon_1) &amp; \\sigma^2_2 &amp; \\cdots &amp; Cov(\\epsilon_2, \\epsilon_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov(\\epsilon_n, \\epsilon_1) &amp; \\cdots &amp; \\cdots &amp; \\sigma^2_n \\end{bmatrix} \\] 8.1 Estimation: GLS Then we have the GLS Estimator: \\[ \\hat{\\beta_{GLS}} = (\\mathbf{X^T \\Sigma^{-1} X})^{-1}\\mathbf{X^T\\Sigma^{-1}Y} \\] If the covariances (off-diagonals) are zero, then we have weighted least squares We can transform the GLS problem into OLS by multiplying \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) by \\(\\mathbf{\\Sigma}^{-1/2}\\) Then we can use the OLS equation on these new, transformed data. ie, \\(\\mathbf{X^*} = \\mathbf{\\Sigma^{-1/2}X}\\), \\(\\mathbf{Y^*} = \\mathbf{\\Sigma^{-1/2}Y}\\). \\(\\hat{\\beta_{GLS}} = \\mathbf{(X^{*T}X^*)^{-1}X^*Y^*}\\) 8.2 GLS example: Phylogenetic autocorrelation One problem that we have is that the covariances are unknown. Adding more data (the favorite method for statisticians!) does not help, as it just adds more covariances to the problem. We have 2 possible solutions. One, model the covariances using a reduced number of parameters (see the Lynx example below). The other solution is to use additional information from outside of the problem. An example is regression accounting for correlated data due to phylogenetic effects. We will use data from the ade4 package on the body size and home range size of different species of carnivores. The data set also includes a phylogeny for the carnivores. library(ape) ## phylogenetic analyses library(ade4) ## source of data library(ggtree) ## tree ploting functions data(carni70) ## the data and tree tr &lt;- read.tree(text=carni70$tre) ## extract the tree LogRange &lt;- log(carni70$tab$range) ## log transform LogSize &lt;- log(carni70$tab$size) dat &lt;- data.frame(LogRange=LogRange, LogSize=LogSize, Species=gsub(&quot;_&quot;, &quot;.&quot;, rownames(carni70$tab))) ## set up the data frame ggplot(aes(x=LogSize, y=LogRange), data=dat)+geom_point() ## plot the data ggtree(tr) + geom_tiplab() ## plot the tree 8.3 GLS example: Phylogenetic autocorrelation Here we fit the OLS model and also the GLS model, to compare the results. Note that both models fit a line of best fit, but with different assumptions. We plot the data and the lines on the same graph: library(nlme) library(ggplot2) fit.lm &lt;- lm(LogRange ~ LogSize, data=dat) ## fit the model using OLS fit.gls &lt;- gls(LogRange ~ LogSize, correlation=corBrownian(phy=tr, form=~Species), data=dat) ## fit the model using GLS summary(fit.lm) ## ## Call: ## lm(formula = LogRange ~ LogSize, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6043 -0.9277 0.3178 1.0636 2.3245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.3562 0.2123 6.389 1.76e-08 *** ## LogSize 0.2793 0.1070 2.611 0.0111 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.415 on 68 degrees of freedom ## Multiple R-squared: 0.09112, Adjusted R-squared: 0.07776 ## F-statistic: 6.818 on 1 and 68 DF, p-value: 0.0111 summary(fit.gls) ## Generalized least squares fit by REML ## Model: LogRange ~ LogSize ## Data: dat ## AIC BIC logLik ## 294.6851 301.3436 -144.3425 ## ## Correlation Structure: corBrownian ## Formula: ~Species ## Parameter estimate(s): ## numeric(0) ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 1.2515297 2.6279314 0.4762414 0.6354 ## LogSize 0.3710265 0.1950539 1.9021745 0.0614 ## ## Correlation: ## (Intr) ## LogSize -0.16 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -0.7127592 -0.1633996 0.0533231 0.1993733 0.5433174 ## ## Residual standard error: 4.968733 ## Degrees of freedom: 70 total; 68 residual ## construct a function to calculate predicted values for various models predictvals &lt;- function(model, xvar, yvar, xrange=NULL, samples=100,...){ if(is.null(xrange)){ if(class(model) %in% c(&#39;lm&#39;,&#39;glm&#39;)){ xrange= range(model$model[[xvar]]) } else if(class(model) %in% c(&#39;loess&#39;)){ xrange = range(model$x) } } ## construct a new data frame for predictions newdata = data.frame(x = seq(xrange[1], xrange[2], length.out = samples)) names(newdata) = xvar newdata[[yvar]] = predict(model, newdata=newdata, ...) newdata } dat &lt;- data.frame(LogRange = LogRange, LogSize = LogSize) ## Use the predictvals function from &quot;R Graphics Cookbook&quot; ols_predicted &lt;- predictvals(fit.lm, &quot;LogSize&quot;, &quot;LogRange&quot;) ols_predicted$fit &lt;- &quot;OLS&quot; gls_predicted &lt;- predictvals(fit.gls, &quot;LogSize&quot;, &quot;LogRange&quot;, xrange = range(dat$LogSize)) gls_predicted$fit &lt;- &quot;GLS&quot; ggplot(aes(x = LogSize, y = LogRange), data = dat)+ geom_point() + geom_line(aes(x = LogSize, y = LogRange, col = fit), data = ols_predicted, linewidth = 2) + geom_line(aes(x = LogSize, y = LogRange, col = fit), data = gls_predicted, linewidth = 2) plot(fit.gls) qqnorm(fit.gls, form = ~resid(., type = &quot;n&quot;), abline = c(0, 1)) 8.4 GLS example: Time Series Frequently we do not have extra information to specify the covariances among the data points. In that case, an alternative is to model the covariances using a reduced model with fewer parameters. This is what is done using time-series analysis. Time-series analysis is useful when data does not come in all at once, but comes in at a regular rate over time. We expect data that are close by (in time) to be likely to be correlated, and data that are more distant (in time) to be less correlated, or even negatively correlated. There may also be strong positive correlations among data that are distant if there are seasonal effects, for example. A simple example of time-series analysis is the autoregression model: - \\(x_t=\\phi_1x_{t-1}+ \\phi_2x_{t-2} + \\dots + \\phi_px_{t-p}+\\epsilon\\) Here, we model the \\(x_t\\) data at time \\(t\\) by regressing the data on itself at different time lags. The \\(\\phi\\)s represent the strength of the regression relationships and the data at different time lags. The challenge is to estimate the \\(\\phi\\)s and determine how many terms we need to successfully model the data. 8.5 Lynx Data The Lynx data are some of the most famous in ecology. They are the number of lynx (Lynx pardinus: Felidae) pelts collected by Canadian hunters and sold to the Hudson Bay Trading Company over more than 100 years. The data describe cycles of small and large populations of lynx over time. The cause of these cycles has been a popular research topic for many years. data(lynx) str(lynx) ## Time-Series [1:114] from 1821 to 1934: 269 321 585 871 1475 ... lynxdat &lt;- data.frame(Year=1821:1934, Lynx=lynx) ggplot(lynxdat, aes(x=Year, y=Lynx)) + geom_line()+geom_point() + scale_x_continuous() + scale_y_continuous() + ggtitle(&quot;Hudson Bay Lynx Returns&quot;)+theme(plot.title = element_text(hjust = 0.5)) 8.6 ACF and PACF The Autocorrelation Function and the Partial Autocorrelation Function (ACF and PCF) describe the pattern of correlatedness of the data at different time lags. In general significant correlations in the ACF (values that fall above or below the horizontal lines) determines whether you have a problem with temporal autocorrelation. The PACF plot tells you how many \\(\\phi\\) parameters you should use in the analysis. It should be noted that the statistical study of time series is a very large topic and mostly beyond the scope of this course. Nevertheless, GLS is a popular method for simple time series. We see in the following plots that there is indeed a problem with temporal autocorrelation. The ACF shows alternating groups of positive and negative correlations. The PACF suggests that 8 \\(\\phi\\) parameters are needed to model the data, because the largest significant lag is lag number 8. library(forecast) print(ggAcf(lynx)) print(ggPacf(lynx)) 8.7 Autoregression of Lynx Data We can use the method of Maximum Likelihood Estimation to estimate the \\(\\phi\\) parameters. The ar function estimates that 8 parameters are necessary, and gives the values of the \\(\\phi\\) coefficients. # find optimal autoregression size ar(lynx, method=&quot;mle&quot;) ## ## Call: ## ar(x = lynx, method = &quot;mle&quot;) ## ## Coefficients: ## 1 2 3 4 5 6 7 8 ## 1.0555 -0.6298 0.2105 -0.1438 -0.0200 0.0373 -0.2341 0.3322 ## ## Order selected 8 sigma^2 estimated as 616997 8.8 GLS Autoregression model of Lynx data Next, we can use GLS to fit the time-series model, specifying the number of \\(\\phi\\) parameters as p = 8. fit &lt;- gls(Lynx~Year, correlation=corARMA(p=8), data=lynxdat, method=&quot;ML&quot;) fit ## Generalized least squares fit by maximum likelihood ## Model: Lynx ~ Year ## Data: lynxdat ## Log-likelihood: -923.1686 ## ## Coefficients: ## (Intercept) Year ## -1535.301943 1.657011 ## ## Correlation Structure: ARMA(8,0) ## Formula: ~1 ## Parameter estimate(s): ## Phi1 Phi2 Phi3 Phi4 Phi5 Phi6 Phi7 ## 1.05436014 -0.62987055 0.20987093 -0.14453853 -0.01997304 0.03578814 -0.23275963 ## Phi8 ## 0.32892377 ## Degrees of freedom: 114 total; 112 residual ## Residual standard error: 1558.429 Finally, we can re-check the autocorrelation plots of the residuals and verify that there are no further problems with autocorrelation. vals &lt;- resid(fit, type=&quot;n&quot;) print(ggAcf(vals)) print(ggPacf(vals)) "],["chap9.html", "9 Categorical Data Analysis 9.1 Types of Data", " 9 Categorical Data Analysis 9.1 Types of Data There are two basic types of data, continuous and discrete. Continuous measurements are on some kind of scale, for example an interval scale, where distances between values on the scale but ratios are not, for example temperature in degrees Celsius. It makes sense to say that two temperatures are different, but it makes no sense to calculate the ratio of temperatures on the Celsius scale. Another possibility is the ratio scale, where there is a meaningful zero point. Examples include body mass and temperature in degrees Kelvin. Other types of continuous data include circular data, such as angles or compass bearings. In this chapter we will discuss the analysis of discrete data. Discrete data can be ordinal or ranked, such as League tables in sporting contests. Or they may be nominal or categorical, falling into separate, unordered, categories. "],["multinomial-experiments.html", "10 Multinomial Experiments 10.1 Example 1. Specified Cell Probabilities 10.2 Example 2: Goodness-of-fit 10.3 Example 3: Contingency Tables 10.4 Assumptions of the \\(X^2\\) Test 10.5 Loglinear Models 10.6 Generalised Linear Models 10.7 The General Linear Model for Normal Responses 10.8 The Poisson GLM 10.9 Fitting the Poisson GLM 10.10 Mode Diagnostics 10.11 The Binomial GLM 10.12 Example: Beetle mortality to insecticide 10.13 Diagnostics 10.14 Another approach to Diagnostics 10.15 Some theory 10.16 Simple example: The Poisson distribution 10.17 Issues when fitting with GLMs", " 10 Multinomial Experiments The simplest type of experiment that leads to categorical data is the multinomial experiment. A multinomial experiment consists of \\(n\\) identical trials. The outcome of each trial falls into one of k categories or cells. The probability that the outcome of a single trial will fall in a particular cell, say, cell i is \\(p_i\\) , where \\(i = 1, 2, \\dots , k\\), and remains the same from trial to trial. Note that: \\[p_1 + p_2 + p_3 + \\dots + p_k = 1\\] (This is an example of the Law of Total Probability (LOTP). In the end, all the probabilities in a problem have to sum to one. Remembering this rule can help you solve many statistical problems.) In a multinomial experiment, the trials are independent. We are interested in the \\(n_1, n_2, n_3, \\dots, n_k\\) where \\(n_i\\) for \\(i=1,2, \\dots, k\\) is equal to the number of trials which the outcome falls into cell \\(i\\). Note that \\(n_1+n_2+n_3+ \\dots +n_k=n\\) The experiment can be modelled using a multinomial distribution with probability mass function: $ \\[p(n_1, n_2, \\dots, n_k)=\\frac{n!}{n_1!n_2 \\dots n_k!}p_1^{n_1}p_2^{n_2} \\dots p_k^{n_k} \\] ## Pearson’s \\(X^2\\) test [Karl Pearson (1857-1936)]{Karl_Pearson.jpg} The \\(X^2\\) test was proposed in 1900, and can be used for testing hypotheses about \\(p_1, p_2, p_3, \\dots, p_k\\) Suppose that \\(n=100\\) balls were thrown into \\(k\\) boxes. Further suppose that \\(p_1=0.1\\). What would the expected number of balls in box 1 be? This would just be \\(E(n_1)=np_1=(100)(0.1)=10\\) Similarly for the other boxes, we have: \\(E(n_i)=np_i, i= 1, 2, \\dots, k\\) Suppose we postulate values for the \\(p_i\\)s. If our hypothesis is correct, the \\(n_i\\)s should not depart much from the expected values (remember residuals in regression? This is the same principle.) Hence it seems reasonable to use the deviations from the expected values in a test statistic: \\(n_i-np_i, i=1 ,2, \\dots, k\\) The test can then be constructed as: \\[\\displaystyle X^2=\\sum_{i=1}^k\\frac{(n_i-np_i)^2}{np_i}=\\sum_{i=1}^k\\frac{[n_i-E(n_i)]^2}{E(n_i)}\\] Asymptotically (for large \\(n\\)), \\(X^2\\) follows a \\(\\chi^2_{\\nu}\\) distribution with \\(\\nu\\) degrees of freedom. The \\(\\chi^2\\) distibution as Probability density function \\[f(y)=\\frac{(y)^{\\nu/2-1}e^{-y/2}}{2^{\\nu/2}\\Gamma(\\nu/2)}, y &gt; 0\\] It’s easier to visualise the \\(\\chi^2\\) distribution: library(ggplot2) ggplot(data.frame(x=c(0, 80)), aes(x)) + stat_function(fun = function (x) dchisq(x, df = 1), geom = &quot;line&quot;, aes(colour = &quot;1&quot;)) + stat_function(fun = function (x) dchisq(x, df = 5), geom = &quot;line&quot;, aes(colour = &quot;5&quot;)) + stat_function(fun = function (x) dchisq(x, df = 50), geom = &quot;line&quot;, aes(colour = &quot;50&quot;)) + ylab(&quot;Density&quot;) + scale_colour_manual(&quot;Degrees of Freedom&quot;, values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), breaks=c(&quot;1&quot;, &quot;5&quot;, &quot;50&quot;)) 10.1 Example 1. Specified Cell Probabilities Frequently we know what the cell probabilities (the \\(p_i\\)’s) should be, based on a scientific theory. For example, Mendelian genetic theory predicts that peas falling into the following classes are distributed 9:3:3:1 with the following phenotypes: Round/Yellow, Wrinkled/Yellow, Round/Green, Wrinkled/Green. Say, N = 100 peas are examined, with the following counts: counts &lt;- c(56, 19, 17, 8) names(counts) &lt;- c(&quot;RY&quot;, &quot;WY&quot;, &quot;RG&quot;, &quot;WG&quot;) counts ## RY WY RG WG ## 56 19 17 8 \\end{itemize} \\end{itemize} \\end{frame} ratio &lt;- c(9, 3, 3, 1) p &lt;- ratio / sum(ratio) expected &lt;- sum(counts) * p x2 &lt;- sum((counts - expected)^2 / expected) x2 ## [1] 0.6577778 pchisq(x2, df=3, lower.tail=FALSE) ## [1] 0.8830872 Because the expected proportions come from theory, and not the data themselves, we do not lose any degrees of freedom apart from the restriction that \\(n_1+n_2+n_3+n_4=100\\). Therefore there are \\(k-1=4-1=3\\) degrees of freedom. Hence, because the p-value is &gt;&gt; 0.05, we do not reject the null hypothesis that the peas follow the Mendelian model. Of course, we can easily do this using the built-in chisq.test() function in R: chisq.test(counts, p = ratio, rescale.p = TRUE) ## ## Chi-squared test for given probabilities ## ## data: counts ## X-squared = 0.65778, df = 3, p-value = 0.8831 10.2 Example 2: Goodness-of-fit The \\(X^2\\) test is also useful for testing whether data are consistent with a particular probability distribution. The idea is to use \\(X^2\\) to measure the fit of the probability model to the data. Significant lack of fit implies rejection of the null hypothesis of no departure from the model. For example, the Poisson distribution is often used to model “count” data. Consider the following data: dat &lt;- data.frame(birds=0:5, frequency=c(16, 19, 9, 4, 2, 0)) dat ## birds frequency ## 1 0 16 ## 2 1 19 ## 3 2 9 ## 4 3 4 ## 5 4 2 ## 6 5 0 Note that birds = 5 really means 5 or greater. The expected number of birds per site is just: \\[ \\hat{\\lambda}=\\frac{(0)(16) + (1)(19) + (2)(9) + (3)(4) + (4)(2)}{16 + 19 + 9 + 4 +2 }=1.14 \\] Aside: Note that the numerator consists of additions and multiplications. This strongly suggests that we could summarise the numerator by using vector multiplication: lambdahat &lt;- with(dat, (birds %*% frequency) / sum(frequency)) lambdahat ## [,1] ## [1,] 1.14 The cell probabilities are just: \\[\\begin{align*} p(y \\mid \\lambda) &amp;= \\frac{\\lambda^y e^{-\\lambda}}{y!}, y= 0, 1, 2, \\dots \\\\ p_0 &amp;= e^{-\\lambda} = 0.3198 \\\\ p_1 &amp;= \\lambda e^{-\\lambda} = 0.3646 \\\\ p_2 &amp;= \\frac{\\lambda^2e^{-\\lambda}}{2} = 0.2078 \\\\ p_3 &amp;= \\frac{\\lambda^3e^{-\\lambda}}{6} = 0.0790 \\\\ p_{4+} &amp;= 1 - p_0 - p_1 - p_2 - p_3 = 0.0288 \\; \\text{(LOTP)} \\end{align*} \\] The expected values for each cell are just: \\[ \\begin{align*} E(n_0) &amp;=(50)(0.3198)=15.9910\\\\ E(n_1) &amp;=(50)(0.3646)=18.2297\\\\ E(n_2) &amp;=(50)(0.2078)=10.3909\\\\ E(n_3) &amp;=(50)(0.0790)=3.9486\\\\ E(n_{4+}) &amp;= (50)(0.0288)=1.4399 \\end{align*} \\] Note that \\(E(n_3)\\) and \\(E(n_{4+})\\) are both \\(&lt; 5\\). The \\(X^2\\) approximation to the \\(\\chi^2\\) distribution is unreliable for such low values. Hence, we should pool these last 2 categories: \\(E(n_{3+})=3.9486+1.4399=5.3884\\) And we now have \\(k=4\\) categories. We now have to Calculate \\(X^2\\): \\[ \\begin{align*} X^2&amp;=\\sum_{i=1}^k\\frac{(n_i-np_i)^2}{np_i}\\\\ &amp;=\\sum_{i=1}^k\\frac{[n_i-E(n_i)]^2}{E(n_i)}\\\\ &amp;= 0.2882\\\\ df &amp;= k-1-1=4-2=2 \\\\ p&amp;=\\chi^2_2(0.4373)=0.8658 \\gg 0.05 \\end{align*} \\] Therefore we do not reject the null hypothesis that the data come from a Poisson distribution. Here is the above analysis in R: probs &lt;- c(dpois(0:2, lambdahat), 1-sum(dpois(0:2, lambdahat))) probs ## [1] 0.3198190 0.3645937 0.2078184 0.1077689 pooled &lt;- c(dat$frequency[1:3], sum(dat$frequency[4:6])) pooled ## [1] 16 19 9 6 expecteds &lt;- sum(pooled) * probs X2 &lt;- sum((pooled-expecteds)^2/expecteds) X2 ## [1] 0.2881509 pchisq(X2, df=2, lower.tail=FALSE) ## [1] 0.8658224 10.3 Example 3: Contingency Tables We can use the \\(X^2\\) test to analyse cross-classified tabular data. The most common use of \\(X^2\\) is a test of independence of rows from columns. ie a test of association between rows and columns}. Here’s and example: Data on Students Publishing (continued below)   High input from Supervisor Supervisor 1st, Student mandatory 2nd 19 Student 1st, Supervisor mandatory 2nd 19 Student 1st, Supervisor courtesy 2nd 3   Medium Input Low Input Supervisor 1st, Student mandatory 2nd 6 2 Student 1st, Supervisor mandatory 2nd 41 27 Student 1st, Supervisor courtesy 2nd 7 31 The data are numbers of published papers by students, cross-classified by the order of authorship and the input from the supervisor. Usually, the author order in biology is to have the student first, except when there is a high level of input from the supervisor. ie when the student is a low contributor to the study. We need to estimate the expected values (under the null hypothesis) in each cell. This is done by multiplying the appropriate row and column totals and dividing by the grand total: \\(\\widehat{E(n_{ij})}=\\frac{r_ic_j}{n}\\). We can then use the usual formula for \\(X^2\\), with degrees of freedom \\((nrow - 1)(ncol - 1)\\). Here’s how to do it in R: mat &lt;- matrix(c(19, 6, 2, 19, 41, 27, 3, 7, 31), nrow=3, byrow=TRUE) csum &lt;- colSums(mat) rsum &lt;- rowSums(mat) expecteds &lt;- outer(rsum, csum) / sum(rsum) X2 &lt;- sum((mat - expecteds)^2 / expecteds) X2 ## [1] 57.36176 pchisq(X2, df = 4, lower.tail = FALSE) ## [1] 1.038795e-11 Therefore, we reject the null hypothesis. There is an association between authorship order and the amount of input by the supervisor. Of course, it is easy to use the chisq.test() function in R: chisq.test(mat) ## ## Pearson&#39;s Chi-squared test ## ## data: mat ## X-squared = 57.362, df = 4, p-value = 1.039e-11 10.4 Assumptions of the \\(X^2\\) Test The data must satisfy the assumptions of a multinomial experiment (independent trials, constant cell probabilities etc.) Expected cell values should not be \\(&lt; 5\\). In this case the asymptotic approximation to the \\(\\chi^2\\) doesn’t hold. You can use other methods or use simulation (See ?chisq.test) The degrees of freedom for a contingency table = (r-1)(c-1) 10.5 Loglinear Models Another approach, which works well for multiway tables, that is, tables with greater than 2 dimensions is the technique of log-linear modelling. Here is the 2-way table described above, analysed using a loglinear model. Notice that the loglm function has a formula argument that looks very similar to the formula argument for aov() and lm(). Note also that the output includes the value of the Pearson \\(X^2\\) statistic. library(MASS) dat &lt;- as.data.frame.table(mat) names(dat) &lt;- c(&quot;Authorship&quot;, &quot;Input&quot;, &quot;Frequency&quot;) dat ## Authorship Input Frequency ## 1 A A 19 ## 2 B A 19 ## 3 C A 3 ## 4 A B 6 ## 5 B B 41 ## 6 C B 7 ## 7 A C 2 ## 8 B C 27 ## 9 C C 31 fit &lt;- loglm(Frequency ~ Authorship + Input, data = dat) fit ## Call: ## loglm(formula = Frequency ~ Authorship + Input, data = dat) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 54.54193 4 4.052392e-11 ## Pearson 57.36176 4 1.038791e-11 10.6 Generalised Linear Models The most flexible approach to modelling non-Normal data is the framework of Generalised Linear Models. GLMs allow the fitting of linear models using a large variety of distributions, including the Normal distribution. For our purposes, we will use GLMs to analyse categorical data and binary data. First we need to discuss some theory, so we can relate the GLM framework to the ordinary linear model framework. This will allow us to see that GLMs are really just an extension of the linear models that we understand already. First, we set up the mathematical formulation. 10.7 The General Linear Model for Normal Responses Recall that the ordinary linear model can be described in the following way: \\[ \\mathbf{Y}=\\mathbf{X}\\beta + \\epsilon, \\epsilon \\sim N(0, \\sigma^2\\mathbf{I}) \\] This can also be written as: \\[ E(\\mathbf{Y}) = \\mathbf{\\mu} = \\mathbf{X}\\beta, \\mathbf{Y} \\sim N(\\mathbf{\\mu}, \\sigma^2\\mathbf{I}) \\] Or more generally: \\[ g(\\mathbf{\\mu}) = \\mathbf{X\\beta},\\; \\mathbf{\\mu} \\sim f(\\mathbf{\\mu, \\theta}) \\] \\(g(.)\\) is called the link function. For normally-distributed data, the link function is the identity function. ie \\(f(x) = x\\). \\(\\mathbf{X\\beta}\\) is the linear predictor, the same as for the ordinary linear model. \\(f(.)\\) is a probability distribution from the exponential family of distributions (more on those later!). \\(\\mu\\) is the Expectation of \\(\\mathbf{Y}\\) ie the mean values. \\(E(\\mathbf{Y})=\\mu\\) Generalised Linear Models extend the General Linear Model in 2 directions: The response variable can have a distribution other than the Normal distribution The relationship between the response and explanatory variables need not be linear (on the scale of the response). That is, the link function allows us to use nonlinear functions of the linear predictor, which can be very useful. 10.8 The Poisson GLM One example is the Poisson Generalized Linear Model, which has many uses including the modelling of count data: \\[log(E(\\mathbf{Y}))=\\mathbf{X\\beta}, \\; \\mathbf{Y} \\sim Poisson(\\lambda)\\] Note that the loglinear model is a special case of the Poisson GLM. The advantage of using the Poisson GLM instead of a loglinear model is that the Poisson GLM can incoporate continuous covariate, whereas the loglinear modelling formulation only works for categorical variables. 10.9 Fitting the Poisson GLM We shall fit the Poisson GLM to the previous contingency table data. fit.full &lt;- glm(Frequency ~ Authorship * Input, data = dat, family = poisson) fit.noint &lt;- glm(Frequency ~ Authorship + Input, data = dat, family = poisson) anova(fit.noint, fit.full, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: Frequency ~ Authorship + Input ## Model 2: Frequency ~ Authorship * Input ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 4 54.542 ## 2 0 0.000 4 54.542 4.052e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Compare with the loglinear model output: fit ## Call: ## loglm(formula = Frequency ~ Authorship + Input, data = dat) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 54.54193 4 4.052392e-11 ## Pearson 57.36176 4 1.038791e-11 Notice that we have constructed a likelihood ratio \\(X^2\\) test by comparing Poisson GLMs, one with an interaction term and one without. The statistic is equal to 54.542 with a p-value close to zero. Notice this is the same as the Likelihood Ratio statistic from the loglinear model! So there is a close mathematical relationship between Poisson GLMs, loglinear models, and the Pearson \\(X^2\\) statistic. This commonality of the underlying mathematics makes it clear that choosing an analysis method for count data is really just a convenience. For many situations, you could use any of the 3 methods (contingency tables, loglinear models, Poisson GLM). 10.10 Mode Diagnostics Model diagnostics for Generalised Linear Models work in a similar way to ordinary linear models, except they are sometimes harder to interpret. Importantly, on the scale of the link function (which is usually the log function for a Poisson GLM), there should be no relationship between the residuals and the fitted values, corresponding to the assumption of homoskedasticity in the general linear model. Similarly, the residuals should be approximately Normal on the scale of the link function. par(mfrow = c(2, 2)) plot(fit.noint) 10.11 The Binomial GLM The Binomial GLM is useful for modelling data where the response variable has 2 categories, for example survival (lived/died), sex ratio (male/female), response to a drug (yes/no), birds in a forest fragment (present/absent) or exams (pass/fail). Poisson and Binomial GLMs are probably the most often used in the analysis of biological data. The Binomial GLM has the following form: \\[g(E(\\mathbf{Y}))= \\mathbf{X\\beta}, \\; \\mathbf{Y} \\sim Binomial(n, p)\\] \\(g(.)\\) is the link function. It can take several forms. It links the Expected value of the response (\\(\\mathbf{Y}\\)) to the linear predictor. The canonical link function for the binomial distribution is the logit function: \\[logit(p)=\\log\\left(\\frac{p}{(1-p)}\\right)\\] 10.12 Example: Beetle mortality to insecticide Groups of beetles were exposed to different concentrations of insecticide, and the number that survived or killed by the insecticide was recorded. A binomial GLM was used in R to model the relationship between survival and insecticide dose: Experiment &lt;- data.frame(Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839), Number.Exposed = c(59, 60, 62, 56, 63, 59, 63, 60), Number.Killed = c(6, 13, 18, 28, 52, 53, 61, 60)) fit &lt;- glm(cbind(Number.Exposed - Number.Killed, Number.Killed) ~ Dose, data = Experiment, family = binomial(link=logit)) summary(fit) ## ## Call: ## glm(formula = cbind(Number.Exposed - Number.Killed, Number.Killed) ~ ## Dose, family = binomial(link = logit), data = Experiment) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6622 -1.0517 -0.6809 0.3001 1.5211 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 59.574 5.078 11.73 &lt;2e-16 *** ## Dose -33.618 2.853 -11.78 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 278.5592 on 7 degrees of freedom ## Residual deviance: 9.8783 on 6 degrees of freedom ## AIC: 40.674 ## ## Number of Fisher Scoring iterations: 4 We can see that there is a strong negative relationship between survivorship (as a proportion) and insecticide dose, as measured by the Dose coefficient in the model summary. But before going further, we should look at some diagnostic plots to see if our model appears reasonable. We see that the sample size (the number of groups exposed to insecticide) is quite small, but there seems to be no strong departures from the usual assumptions of no relationship between the residuals and fitted values, and the normality of the residuals (on the scale of the link function, which in this case is the logit() function). 10.13 Diagnostics par(mfrow = c(2, 2)) plot(fit) 10.14 Another approach to Diagnostics The plot of the residuals versus the fitted values for GLMs can be hard to interpret, as the data are non-normal, or not even continuous. A package that can provide diagnostic plots that are interpretable in the same way as the general linear model is the DHARMa package. It works by simulating residuals from the model and plotting these standardised, simulated residuals using the usual plots. Here is an example: library(DHARMa) res &lt;- simulateResiduals(fit, plot=TRUE) Notice that the left-hand plot looks like the usual quantile-quantile plot, but with the results of some extra tests plotted on it. In fact, this is actually a quantile-quantile plot based on the Uniform distribution. The right-hand plot is like the residuals versus the fitted values. You can see that this plot has a hump in it, which was not obvious from the ordinary diagnostic plots. This suggests that the data may actually follow a parabolic curve, which is described by a quadratic polynomial. We can fit a model with both a linear term and a quadratic term, and see if it is better than the model with just the linear term. fit2 &lt;- glm(cbind(Number.Exposed - Number.Killed, Number.Killed) ~ poly(Dose,2), data = Experiment, family = binomial(link=logit)) summary(fit2) ## ## Call: ## glm(formula = cbind(Number.Exposed - Number.Killed, Number.Killed) ~ ## poly(Dose, 2), family = binomial(link = logit), data = Experiment) ## ## Deviance Residuals: ## 1 2 3 4 5 6 7 8 ## 0.3262 -0.7598 0.3934 0.5521 -1.0252 0.5541 0.3984 -0.8577 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.0315 0.2044 -5.047 4.48e-07 *** ## poly(Dose, 2)1 -6.6299 0.6423 -10.323 &lt; 2e-16 *** ## poly(Dose, 2)2 -1.3391 0.5465 -2.450 0.0143 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 278.5592 on 7 degrees of freedom ## Residual deviance: 3.3958 on 5 degrees of freedom ## AIC: 36.192 ## ## Number of Fisher Scoring iterations: 4 We see that indeed there is a significant 2nd order polynomial term (\\(p = 0.0143\\)). So it looks like the polynomial is a better fit compared to the plain linear model. However, there is still a strong linear component, although highly significant, the effect size is not as great (-6.6299). 10.15 Some theory The theory for GLMs was first worked out for exponential family distributions. These are distributions which can have their probability density (mass) functions written in the following form: \\[f(y\\mid\\theta)= e^{a(y)b(\\theta)+c(\\theta)+d(y)}\\] where y is the data and \\(\\theta\\) is a vector of parameters. \\(b(\\theta)\\) is known as the natural parameter and defines the canonical link function. If \\(a(y) = y\\), then the function is said to be in canonical form. 10.16 Simple example: The Poisson distribution The Poisson distribution has probability mass function: \\[f(y\\mid \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\; y = 0, 1, 2, \\ldots\\] It can be written in exponential form as: \\[f(y \\mid \\lambda) = \\exp(y \\log \\lambda - \\lambda - \\log y!)\\] Note that \\(a(y) = y\\) so the distribution is in canonical form. Also, the natural parameter \\(b(\\lambda) = \\log \\lambda\\) so \\(\\log()\\) is the canonical link function. ##The Binomial distribution in exponential form The Binomial distribution has probability mass function: \\[f(y \\mid p) = \\binom{n}{y} p^y(1-p)^{n-y}\\] In exponential form: \\[ \\begin{align*} f(y \\mid p) &amp;= \\exp \\left[ y \\log p - y \\log(1-p)+n \\log(1-p) + log \\binom{n}{y} \\right] \\\\ &amp;= \\exp \\left[ y \\log \\frac{p}{1-p}+ n \\log(1-p) + log \\binom{n}{y} \\right] \\end{align*} \\] which is in canonical form and the natural parameter is \\(\\log \\frac{p}{(1-p)}\\). 10.17 Issues when fitting with GLMs One problem that can occur is overdispersion. This is particularly common with Poisson models. Overdispersion occurs when the variance of the data is greater than the mean. Since in a Poisson model, the variance equals the mean, this is a strong restriction. There are several solutions, including fitting by a technique called quasilikelihood, which is beyond the scope of this course. An alternative approach which tends to work much better is to look for a discrete distribution that can model the overdispersion. In practice, the Negative Binomial distribution often works quite well. The Negative Binomial is a discrete distribution, taking values 0, 1, 2, … so it is suitable for count data. Another problem that can occur, particularly in count data is zero inflation. When using a Poisson model, it is expected to get at least a few zeros in the data set. However, it is often found that there are way too many zeros than predicted by a Poisson distribution: dat &lt;- data.frame(Poisson=c(rpois(50, lambda=5), rep(0, 50))) with(dat, hist(Poisson, main = &quot;Zero Inflation&quot;)) In this case, a zero-inflated Poisson model (ZIP) might work better, or a hurdle model. Again, these are beyond the scope of the course. Another issue is how to choose an appropriate link function. While the canonical link function usually performs pretty well, other link functions are possible. For a binomial model, R provides the logit, probit, cauchit, log, and complementary log-log. See ?family for more details. Different link functions may model different regions of the data better. The choice of distribution family is crucial. Generally for count data, a Poisson distribution is the first port of call, but be prepared to change it (see above). For binary data (e.g. presence/absence or alive/died etc.) the Binomial distribution is usually appropriate. For continuous data, the Normal distribution usually performs well but that leads us straight back to the general linear model. In situations where this skew in the data, a Gamma distribution model may be appropriate. Other distributions can be used other than the Exponential family distributions, but the mathematical foundations of the GLM theory are not as strong. Another issue is the inclusion of random effects (e.g. blocking) in the model. This leads directly to Generalised Linear Mixed Models (GLMM). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
